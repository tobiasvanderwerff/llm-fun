{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea: ask questions to an LLM about a codebase, using embeddings. Feeding the entire codebase to the LLM would exceed it's maximum context length, so embeddings are used to select those pieces of text which are most relevant to the input prompt/question. A single embedding represents a chunk of the codebase, e.g. 1000 tokens of it, turned into a vector representation using OpenAI's embeddings API. \n",
    "\n",
    "The pipeline then goes as follows:\n",
    "- ask a question\n",
    "- match the question to the most similar embeddings (e.g. top 5) \n",
    "- add the pieces of text corresponding to the matched embeddings to the LLM input, along with the question itself\n",
    "- run the LLM\n",
    "\n",
    "Note that a lot hinges on the matched embeddings. If the answer to a question is not contained in the matched embeddings, the LLM has no way to give a correct answer based on its context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv());"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tinygrad codebase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I took all the Python files from https://github.com/geohot/tinygrad and put them in `data/tinygrad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def printw(text):\n",
    "    print(textwrap.fill(text, width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "tinygrad_path = \"data/tinygrad\"\n",
    "\n",
    "# Collect all Python files in the tinygrad repo\n",
    "loader = DirectoryLoader(tinygrad_path, glob=\"**/*.py\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 documents loaded.\n",
      "data/tinygrad/image.py\n",
      "data/tinygrad/jit.py\n",
      "data/tinygrad/ops_clang.py\n",
      "data/tinygrad/tensor.py\n",
      "data/tinygrad/ops.py\n",
      "data/tinygrad/shapetracker.py\n",
      "data/tinygrad/mlops.py\n",
      "data/tinygrad/ops_cpu.py\n",
      "data/tinygrad/helpers.py\n",
      "data/tinygrad/symbolic.py\n",
      "data/tinygrad/cstyle.py\n",
      "data/tinygrad/llvmir.py\n",
      "data/tinygrad/ops_gpu.py\n",
      "data/tinygrad/__init__.py\n",
      "data/tinygrad/graph.py\n",
      "data/tinygrad/lazy.py\n",
      "data/tinygrad/lib.py\n",
      "data/tinygrad/ops_llvm.py\n",
      "data/tinygrad/ops_cuda.py\n",
      "data/tinygrad/linearizer.py\n",
      "data/tinygrad/ops_metal.py\n",
      "data/tinygrad/ops_torch.py\n",
      "data/tinygrad/optim.py\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(documents)} documents loaded.\")\n",
    "for doc in documents:\n",
    "    print(doc.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1459, which is longer than the specified 1000\n",
      "Created a chunk of size 1005, which is longer than the specified 1000\n",
      "Created a chunk of size 1959, which is longer than the specified 1000\n",
      "Created a chunk of size 2511, which is longer than the specified 1000\n",
      "Created a chunk of size 1064, which is longer than the specified 1000\n",
      "Created a chunk of size 1056, which is longer than the specified 1000\n",
      "Created a chunk of size 1247, which is longer than the specified 1000\n",
      "Created a chunk of size 1169, which is longer than the specified 1000\n",
      "Created a chunk of size 1199, which is longer than the specified 1000\n",
      "Created a chunk of size 1407, which is longer than the specified 1000\n",
      "Created a chunk of size 1124, which is longer than the specified 1000\n",
      "Created a chunk of size 1763, which is longer than the specified 1000\n",
      "Created a chunk of size 1632, which is longer than the specified 1000\n",
      "Created a chunk of size 5008, which is longer than the specified 1000\n",
      "Created a chunk of size 1256, which is longer than the specified 1000\n",
      "Created a chunk of size 1471, which is longer than the specified 1000\n",
      "Created a chunk of size 1132, which is longer than the specified 1000\n",
      "Created a chunk of size 1008, which is longer than the specified 1000\n",
      "Created a chunk of size 1094, which is longer than the specified 1000\n",
      "Created a chunk of size 1215, which is longer than the specified 1000\n",
      "Created a chunk of size 1696, which is longer than the specified 1000\n",
      "Created a chunk of size 1445, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=None, openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6, request_timeout=None, headers=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "docsearch = Chroma.from_documents(docs, embeddings).as_retriever()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "delimiter = \"####\"  # this is 1 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt 1: question answering in JSON format\n",
    "prompt_template = \"\"\"You will receive code questions based on the code snippets below. \\\n",
    "Your goal is to give an answer which is as accurate as possible. \\\n",
    "If the question is of the form '<object> info', where <object> is a class or function, \\\n",
    "answer in JSON format, using the following fields: object, parent, description, parameters, \\\n",
    "return value, and examples. \\\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt 2: chain of thought reasoning\n",
    "prompt_template_cot = f\"\"\"You will receive code questions based on the \\\n",
    "code snippets below. Follow these steps to answer the question. \\\n",
    "The question will be delimited with four hashtags, i.e. {delimiter}. \n",
    "\n",
    "Step 1:{delimiter} First decide whether the user is asking about a class, function, \\\n",
    "or variable.\n",
    "\n",
    "Step 2:{delimiter} If the user is asking about a class or function, \\\n",
    "answer in JSON format, using the following fields: object, parent, \\\n",
    "description, parameters.\n",
    "\n",
    "Step 3:{delimiter} If the user is asking about a variable, \\\n",
    "answer what the variable is used for.\n",
    "\n",
    "Step 4:{delimiter}: If the user made any assumptions, \\\n",
    "figure out whether the assumption is true based on the code snippets.\n",
    "\n",
    "Step 5:{delimiter}: First, politely correct the \\\n",
    "user's incorrect assumptions if applicable. \\\n",
    "Answer the user in a friendly tone.\n",
    "\n",
    "Use the following format:\n",
    "Step 1:{delimiter} <step 1 reasoning>\n",
    "Step 2:{delimiter} <step 2 reasoning>\n",
    "Step 3:{delimiter} <step 3 reasoning>\n",
    "Step 4:{delimiter} <step 4 reasoning>\n",
    "Response to user:{delimiter} <response to user>\n",
    "\n",
    "Make sure to include {delimiter} to separate every step.\n",
    "\"\"\" + \"\"\"\\\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the prompt template to use\n",
    "# PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "PROMPT = PromptTemplate(template=prompt_template_cot, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt is 1031 characters long.\n",
      "\n",
      "=== Prompt template ===\n",
      "\n",
      "You will receive code questions based on the code snippets below. Follow these steps to answer the question. The question will be delimited with four hashtags, i.e. ####. \n",
      "\n",
      "Step 1:#### First decide whether the user is asking about a class, function, or variable.\n",
      "\n",
      "Step 2:#### If the user is asking about a class or function, answer in JSON format, using the following fields: object, parent, description, parameters.\n",
      "\n",
      "Step 3:#### If the user is asking about a variable, answer what the variable is used for.\n",
      "\n",
      "Step 4:####: If the user made any assumptions, figure out whether the assumption is true based on the code snippets.\n",
      "\n",
      "Step 5:####: First, politely correct the user's incorrect assumptions if applicable. Answer the user in a friendly tone.\n",
      "\n",
      "Use the following format:\n",
      "Step 1:#### <step 1 reasoning>\n",
      "Step 2:#### <step 2 reasoning>\n",
      "Step 3:#### <step 3 reasoning>\n",
      "Step 4:#### <step 4 reasoning>\n",
      "Response to user:#### <response to user>\n",
      "\n",
      "Make sure to include #### to separate every step.\n",
      "\n",
      "{context}\n",
      "\n",
      "Question: {question}\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prompt template is {len(PROMPT.template)} characters long.\\n\")\n",
    "print(\"=== Prompt template ===\\n\")\n",
    "print(PROMPT.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0) # 'ada' 'gpt-3.5-turbo' 'gpt-4',\n",
    "qa = load_qa_chain(model, prompt=PROMPT)\n",
    "\n",
    "# qa({\"question\": query, \"chat_history\": []})\n",
    "# docs = docsearch.similarity_search(query)\n",
    "# docs = docsearch.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: What does shape_fxn_for_op represent? \n",
      "\n",
      "Step 1:#### The code snippet represents a dictionary.\n",
      "Step 2:#### The dictionary maps operations (represented by the Op enum) to functions that perform those operations on tensors.\n",
      "Step 3:#### N/A\n",
      "Step 4:#### N/A\n",
      "Response to user:#### The code snippet represents a dictionary that maps operations to functions that perform those operations on tensors.\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    # \"Tensor info\",\n",
    "    # \"What does Function class represent, and why does it inherit from the Tensor class?\",\n",
    "    \"What does shape_fxn_for_op represent?\",\n",
    "    # \"What does the Tensor class represent?\",\n",
    "    # \"What happens when you call `detach()` on a Tensor?\",\n",
    "    # \"How does the Tensor.stack() method work?\",\n",
    "    # \"What classes are derived from the Function class?\",\n",
    "    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n",
    "    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",\n",
    "] \n",
    "\n",
    "for question in questions:  \n",
    "    docs_ = docsearch.get_relevant_documents(question)\n",
    "    result = qa({\"input_documents\": docs_, \"question\": question, \"chat_history\": []})\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(result[\"output_text\"])\n",
    "    # print(f\"**Answer**: {result['answer']} \\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
