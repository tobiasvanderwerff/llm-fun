{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: turn a PDF into a text representation and use embeddings to ask questions about the text (same idea as `ask_a_codebase.ipynb`, but with a bit more processing around it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(verbose=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a paper questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the LoFTR paper (https://arxiv.org/pdf/2104.00680.pdf) as an example paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "    - try using a system prompt to make the model more precise and helpful\n",
    "    - Document postprocessing:\n",
    "        - footnotes are not distinguishable as foot notes. They are simply recognized as being part of the text.\n",
    "        - if there is an image with text in it, that text is also extracted\n",
    "        - check if math formatting can be better\n",
    "            - surround with $ signs?\n",
    "        - idea: add metadata to the chunks, e.g. page number\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def printw(text):\n",
    "    print(textwrap.fill(text, width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 document(s) in the data\n",
      "Characters per document: [3941, 5754, 26674, 21725, 4889, 5142, 5190, 3093, 5524, 3396]\n"
     ]
    }
   ],
   "source": [
    "# Load the pdf\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = \"data/LoFTR_paper.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)  # the pdf is split by page\n",
    "data = loader.load()\n",
    "\n",
    "print(f\"There are {len(data)} document(s) in the data\")\n",
    "print(f\"Characters per document: {[len(d.page_content) for d in data]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some postprocessing to filter out things like SHA1 hashes\n",
    "\n",
    "import re\n",
    "\n",
    "pattern = re.compile(r\"<latexit.*?>.*?</latexit>\", re.DOTALL)\n",
    "for d in data:\n",
    "    d.page_content = pattern.sub(\"\", d.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "LoFTR: Detector-Free Local Feature Matching with Transformers Jiaming Sun1;2\u0003Zehong Shen1\u0003Yuang\n",
      "Wang1\u0003Hujun Bao1Xiaowei Zhou1y 1Zhejiang University2SenseTime Research Abstract We present a novel\n",
      "method for local image feature matching. Instead of performing image feature detection, description,\n",
      "and matching sequentially, we propose to ﬁrst establish pixel-wise dense matches at a coarse level\n",
      "and later reﬁne the good matches at a ﬁne level. In contrast to dense methods that use a cost volume\n",
      "to search corre- spondences, we use self and cross attention layers in Trans- former to obtain\n",
      "feature descriptors that are conditioned on both images. The global receptive ﬁeld provided by\n",
      "Trans- former enables our method to produce dense matches in low-texture areas, where feature\n",
      "detectors usually strug- gle to produce repeatable interest points. The experiments on indoor and\n",
      "outdoor datasets show that LoFTR outper- forms state-of-the-art methods by a large margin. LoFTR\n",
      "also ranks ﬁrst on two public benchmarks of visual local- ization among the published methods. Code\n",
      "is available at our project page: https://zju3dv.github.io/loftr/ . 1. Introduction Local feature\n",
      "matching between images is the corner- stone of many 3D computer vision tasks, including structure\n",
      "from motion (SfM), simultaneous localization and mapping (SLAM), visual localization, etc. Given two\n",
      "images to be matched, most existing matching methods consist of three separate phases: feature\n",
      "detection, feature description, and feature matching. In the detection phase, salient points like\n",
      "corners are ﬁrst detected as interest points from each im- age. Local descriptors are then extracted\n",
      "around neigh- borhood regions of these interest points. The feature de- tection and description\n",
      "phases produce two sets of interest points with descriptors, the point-to-point correspondences of\n",
      "which are later found by nearest neighbor search or more sophisticated matching algorithms. The use\n",
      "of a feature detector reduces the search space of matching, and the resulted sparse correspondences\n",
      "are sufﬁ- cient for most tasks, e.g., camera pose estimation. However, a feature detector may fail\n",
      "to extract enough interest points \u0003The ﬁrst three authors contributed equally. The authors are\n",
      "afﬁliated with the State Key Lab of CAD&CG and ZJU-SenseTime Joint Lab of 3D Vision.yCorresponding\n",
      "author: Xiaowei Zhou. Figure 1: Comparison between the proposed method LoFTR and the detector-based\n",
      "method SuperGlue [37]. This example demonstrates that LoFTR is capable of ﬁnd- ing correspondences\n",
      "on the texture-less wall and the ﬂoor with repetitive patterns, where detector-based methods\n",
      "struggle to ﬁnd repeatable interest points.1 that are repeatable between images due to various\n",
      "factors such as poor texture, repetitive patterns, viewpoint change, illumination variation, and\n",
      "motion blur. This issue is espe- cially prominent in indoor environments, where low-texture regions\n",
      "or repetitive patterns sometimes occupy most areas in the ﬁeld of view. Fig. 1 shows an example.\n",
      "Without re- peatable interest points, it is impossible to ﬁnd correct cor- respondences even with\n",
      "perfect descriptors. Several recent works [34, 33, 19] have attempted to rem- edy this problem by\n",
      "establishing pixel-wise dense matches. Matches with high conﬁdence scores can be selected from the\n",
      "dense matches, and thus feature detection is avoided. However, the dense features extracted by\n",
      "convolutional neu- ral networks (CNNs) in these works have limited receptive ﬁeld which may not\n",
      "distinguish indistinctive regions. In- stead, humans ﬁnd correspondences in these indistinctive\n",
      "regions not only based on the local neighborhood, but with a larger global context. For example,\n",
      "low-texture regions in 1Only the inlier matches after RANSAC are shown. The green color indicates a\n",
      "match with epipolar error smaller than 5\u000210\u00004(in the nor- malized image coordinates).\n",
      "1arXiv:2104.00680v1  [cs.CV]  1 Apr 2021\n",
      "==================\n",
      "==================\n",
      "Fig. 1 can be distinguished according to their relative po- sitions to the edges. This observation\n",
      "tells us that a large receptive ﬁeld in the feature extraction network is crucial. Motivated by the\n",
      "above observations, we propose Lo- cal Feature TRansformer (LoFTR), a novel detector-free approach\n",
      "to local feature matching. Inspired by seminal work SuperGlue [37], we use Transformer [48] with\n",
      "self and cross attention layers to process (transform) the dense local features extracted from the\n",
      "convolutional backbone. Dense matches are ﬁrst extracted between the two sets of transformed\n",
      "features at a low feature resolution ( 1=8of the image dimension). Matches with high conﬁdence are\n",
      "se- lected from these dense matches and later reﬁned to a sub- pixel level with a correlation-based\n",
      "approach. The global receptive ﬁeld and positional encoding of Transformer en- able the transformed\n",
      "feature representations to be context- and position-dependent. By interleaving the self and cross\n",
      "attention layers multiple times, LoFTR learns the densely- arranged globally-consented matching\n",
      "priors exhibited in the ground-truth matches. A linear transformer is also adopted to reduce the\n",
      "computational complexity to a man- ageable level. We evaluate the proposed method on several image\n",
      "matching and camera pose estimation tasks with indoor and outdoor datasets. The experiments show\n",
      "that LoFTR out- performs detector-based and detector-free feature matching baselines by a large\n",
      "margin. LoFTR also achieves state- of-the-art performance and ranks ﬁrst among the published methods\n",
      "on two public benchmarks of visual localization. Compared to detector-based baseline methods, LoFTR\n",
      "can produce high-quality matches even in indistinctive regions with low-textures, motion blur, or\n",
      "repetitive patterns. 2. Related Work Detector-based Local Feature Matching. Detector-based methods\n",
      "have been the dominant approach for local fea- ture matching. Before the age of deep learning, many\n",
      "renowned works in the traditional hand-crafted local fea- tures have achieved good performances.\n",
      "SIFT [26] and ORB [35] are arguably the most successful hand-crafted local features and are widely\n",
      "adopted in many 3D com- puter vision tasks. The performance on large viewpoint and illumination\n",
      "changes of local features can be signif- icantly improved with learning-based methods. Notably, LIFT\n",
      "[51] and MagicPoint [8] are among the ﬁrst success- ful learning-based local features. They adopt\n",
      "the detector- based design in hand-crafted methods and achieve good performance. SuperPoint [9]\n",
      "builds upon MagicPoint and proposes a self-supervised training method through homo- graphic\n",
      "adaptation. Many learning-based local features along this line [32, 11, 25, 28, 47] also adopt the\n",
      "detector- based design.The above-mentioned local features use the nearest neighbor search to ﬁnd\n",
      "matches between the extracted in- terest points. Recently, SuperGlue [37] proposes a learning- based\n",
      "approach for local feature matching. SuperGlue ac- cepts two sets of interest points with their\n",
      "descriptors as input and learns their matches with a graph neural net- work (GNN), which is a\n",
      "general form of Transformers [16]. Since the priors in feature matching can be learned with a data-\n",
      "driven approach, SuperGlue achieves impressive per- formance and sets the new state of the art in\n",
      "local feature matching. However, being a detector-dependent method, it has the fundamental drawback\n",
      "of being unable to detect repeatable interest points in indistinctive regions. The at- tention range\n",
      "in SuperGlue is also limited to the detected interest points only. Our work is inspired by SuperGlue\n",
      "in terms of using self and cross attention in GNN for message passing between two sets of\n",
      "descriptors, but we propose a detector-free design to avoid the drawbacks of feature de- tectors. We\n",
      "also use an efﬁcient variant of the attention lay- ers in Transformer to reduce the computation\n",
      "costs. Detector-free Local Feature Matching. Detector-free methods remove the feature detector phase\n",
      "and directly pro- duce dense descriptors or dense feature matches. The idea of dense features\n",
      "matching dates back to SIFT Flow [23]. [6, 39] are the ﬁrst learning-based approaches to learn\n",
      "pixel- wise feature descriptors with the contrastive loss. Similar to the detector-based methods,\n",
      "the nearest neighbor search is usually used as a post-processing step to match the dense\n",
      "descriptors. NCNet [34] proposed a different approach by directly learning the dense correspondences\n",
      "in an end-to- end manner. It constructs 4D cost volumes to enumer- ate all the possible matches\n",
      "between the images and uses 4D convolutions to regularize the cost volume and enforce neighborhood\n",
      "consensus among all the matches. Sparse NCNet [33] improves upon NCNet and makes it more ef- ﬁcient\n",
      "with sparse convolutions. Concurrently with our work, DRC-Net [19] follows this line of work and\n",
      "proposes a coarse-to-ﬁne approach to produce dense matches with higher accuracy. Although all the\n",
      "possible matches are con- sidered in the 4D cost volume, the receptive ﬁeld of 4D con- volution is\n",
      "still limited to each matches’ neighborhood area. Apart from neighborhood consensus, our work\n",
      "focuses on achieving global consensus between matches with the help of the global receptive ﬁeld in\n",
      "Transformers, which is not exploited in NCNet and its follow-up works. [24] proposes a dense\n",
      "matching pipeline for SfM with endoscopy videos. The recent line of research [46, 45, 44, 15] that\n",
      "focuses on bridging the task of local feature matching and optical ﬂow estimation, is also related\n",
      "to our work. Transformers in Vision Related Tasks. Transformer [48] has become the de facto standard\n",
      "for sequence modeling in natural language processing (NLP) due to their simplic- 2\n",
      "==================\n",
      "==================\n",
      "1. Local Feature CNN IA,IB1/8 1 Self-Attention LayerCross-Attention Layer conﬁdence matrix ˆFA,ˆFB\n",
      "ﬂatten E(·) expectation 4. Coarse-to-Fine Modulecorrelation& softmax w⇥w cropping onˆF ˆiˆjˆj0\n",
      "(˜i,˜j) LoFTR Module(1/8)2HBWB (1/8)2HAWA LoFTRModule3. Matching Module 2. Coarse-Level Local\n",
      "Feature Transform˜FAtr,˜FBtr ˜FA,˜FB Matching LayerDi↵erentiable positionalencoding⇥Nc⇥Nf for every\n",
      "coarse prediction (˜i,˜j)2McPc Mf={(ˆi,ˆj0)}1/2Figure 2: Overview of the proposed method. LoFTR has\n",
      "four components: 1.A local feature CNN extracts the coarse-level feature maps ~FAand~FB, together\n",
      "with the ﬁne-level feature maps ^FAand^FBfrom the image pair IAandIB(Section 3.1). 2.The coarse\n",
      "feature maps are ﬂattened to 1-D vectors and added with the positional encoding. The added features\n",
      "are then processed by the Local Feature TRansformer (LoFTR) module, which has Ncself-attention and\n",
      "cross-attention layers (Section 3.2). 3.A differentiable matching layer is used to match the\n",
      "transformed features, which ends up with a conﬁdence matrix Pc. The matches inPcare selected\n",
      "according to the conﬁdence threshold and mutual-nearest-neighbor criteria, yielding the coarse-level\n",
      "match prediction Mc(Section 3.3). 4.For every selected coarse prediction (~i;~j)2Mc, a local window\n",
      "with size w\u0002wis cropped from the ﬁne-level feature map. Coarse matches will be reﬁned within this\n",
      "local window to a sub-pixel level as the ﬁnal match prediction Mf(Section 3.4). ity and computation\n",
      "efﬁciency. Recently, Transformers are also getting more attention in computer vision tasks, such as\n",
      "image classiﬁcation [10], object detection [3] and seman- tic segmentation [49]. Concurrently with\n",
      "our work, [20] proposes to use Transformer for disparity estimation. The computation cost of the\n",
      "vanilla Transformer grows quadrat- ically as the length of input sequences due to the multipli-\n",
      "cation between query and key vectors. Many efﬁcient vari- ants [42, 18, 17, 5] are proposed recently\n",
      "in the context of processing long language sequences. Since no assumption of the input data is made\n",
      "in these works, they are also well suited for processing images. 3. Methods Given the image pair\n",
      "IAandIB, the existing local fea- ture matching methods use a feature detector to extract in- terest\n",
      "points. We propose to tackle the repeatability issue of feature detectors with a detector-free\n",
      "design. An overview of the proposed method LoFTR is presented in Fig. 2. 3.1. Local Feature\n",
      "Extraction We use a standard convolutional architecture with FPN [22] (denoted as the local feature\n",
      "CNN) to extractmulti-level features from both images. We use ~FAand~FB to denote the coarse-level\n",
      "features at 1=8of the original im- age dimension, and ^FAand^FBthe ﬁne-level features at 1=2 of the\n",
      "original image dimension. Convolutional Neural Networks (CNNs) possess the in- ductive bias of\n",
      "translation equivariance and locality, which are well suited for local feature extraction. The\n",
      "downsam- pling introduced by the CNN also reduces the input length of the LoFTR module, which is\n",
      "crucial to ensure a manage- able computation cost. 3.2. Local Feature Transformer (LoFTR) Module\n",
      "After the local feature extraction, ~FAand~FBare passed through the LoFTR module to extract position\n",
      "and context dependent local features. Intuitively, the LoFTR module transforms the features into\n",
      "feature representations that are easy to match. We denote the transformed features as ~FA tr and~FB\n",
      "tr. Preliminaries: Transformer [48]. We ﬁrst brieﬂy intro- duce the Transformer here as background.\n",
      "A Transformer encoder is composed of sequentially connected encoder lay- ers. Fig. 3(a) shows the\n",
      "architecture of an encoder layer. The key element in the encoder layer is the attention 3\n",
      "==================\n",
      "==================\n",
      "QKV QKV elu(·)+1elu(·)+1 (a)(b) (c) Q=WQfiK=WKfjV=WVfjh ConcatMatMul MatMul MatMulMatMul Softmax\n",
      "Dot-Product AttentionLinear Attention fifj fi Attention LayerDk⇥Dv Nq⇥Nk Cat&Linear&Norm Feed-\n",
      "Forward&NormAddFigure 3: Encoder layer and attention layer in LoFTR. (a) Transformer encoder layer.\n",
      "hrepresents the multiple heads of attention. (b) Vanilla dot-product attention with O(N2)complexity.\n",
      "(c) Linear attention layer with O(N) complexity. The scale factor is omitted for simplicity. layer.\n",
      "The input vectors for an attention layer are conven- tionally named query, key, and value. Analogous\n",
      "to infor- mation retrieval, the query vector Qretrieves information from the value vector V,\n",
      "according to the attention weight computed from the dot product of Qand the key vector K\n",
      "corresponding to each value V. The computation graph of the attention layer is presented in Fig.\n",
      "3(b). Formally, the attention layer is denoted as: Attention (Q;K;V ) = softmax( QKT)V: Intuitively,\n",
      "the attention operation selects the relevant infor- mation by measuring the similarity between the\n",
      "query ele- ment and each key element. The output vector is the sum of the value vectors weighted by\n",
      "the similarity scores. As a result, the relevant information is extracted from the value vector if\n",
      "the similarity is high. This process is also called “message passing” in Graph Neural Network.\n",
      "Linear Transformer. Denoting the length of QandKas Nand their feature dimension as D, the dot\n",
      "product be- tweenQandKin the Transformer introduces computation cost that grows quadratically (\n",
      "O(N2)) with the length of the input sequence. Directly applying the vanilla version of Transformer\n",
      "in the context of local feature matching is im- practical even when the input length is reduced by\n",
      "the local feature CNN. To remedy this problem, we propose to use an efﬁcient variant of the vanilla\n",
      "attention layer in Trans- former. Linear Transformer [17] proposes to reduce the computation\n",
      "complexity of Transformer to O(N)by sub- stituting the exponential kernel used in the original\n",
      "atten- tion layer with an alternative kernel function sim(Q;K ) = \u001e(Q)\u0001\u001e(K)T;where\u001e(\u0001) = elu(\u0001) + 1\n",
      ". This operation is illustrated by the computation graph in Fig. 3(c). Utilizing\n",
      "SelfAttentionCrossAttentionFeature VisualizationLRConv #1Conv #2Conv #3 Transformer\n",
      "#1LRATTENTION(b)(a) (c)Figure 4: Illustration of the receptive ﬁeld of (a)Convolu- tions and\n",
      "(b)Transformers. Assume that the objective is to establish a connection between the L and R elements\n",
      "to extract their joint feature representation. Due to the local- connectivity of convolutions, many\n",
      "convolution layers need to be stacked together in order to achieve this connection. The global\n",
      "receptive ﬁeld of Transformers enables this con- nection to be established through only one\n",
      "attention layer. (c)Visualization of the attention weights and transformed dense features. We use\n",
      "PCA to reduce the dimension of the transformed features ~FA trand~FB trand visualize the results\n",
      "with RGB color. Zoom in for details. the associativity property of matrix products, the multipli-\n",
      "cation between \u001e(K)TandVcan be carried out ﬁrst. Since D\u001cN, the computation cost is reduced to O(N).\n",
      "Positional Encoding. We use the 2D extension of the standard positional encoding in Transformers\n",
      "following DETR [3]. Different from DETR, we only add them to the backbone output once. We leave the\n",
      "formal deﬁnition of the positional encoding in the supplementary material. In- tuitively, the\n",
      "positional encoding gives each element unique position information in the sinusoidal format. By\n",
      "adding the position encoding to ~FAand~FB, the transformed fea- tures will become position-\n",
      "dependent, which is crucial to the ability of LoFTR to produce matches in indistinctive re- gions.\n",
      "As shown in the bottom row of Fig. 4(c), although the input RGB color is homogeneous on the white\n",
      "walls, the transformed features ~FA trand~FB trareunique for each position demonstrated by the\n",
      "smooth color gradients. More visualizations are provided in Fig. 6. Self-attention and Cross-\n",
      "attention Layers. For self- attention layers, the input features fiandfj(shown in Fig. 3) are the\n",
      "same (either ~FAor~FB). For cross-attention layers, the input features fiandfjare either ( ~FAand\n",
      "~FB) or ( ~FBand~FA) depending on the direction of cross- attention. Following [37], we interleave\n",
      "the self and cross attention layers in the LoFTR module by Nctimes. The attention weights of the\n",
      "self and cross attention layers in LoFTR are visualized in the ﬁrst two rows of Fig. 4(c). 4\n",
      "==================\n",
      "==================\n",
      "3.3. Establishing Coarse-level Matches Two types of differentiable matching layers can be ap- plied\n",
      "in LoFTR, either with an optimal transport (OT) layer as in [37] or with a dual-softmax operator\n",
      "[34, 47]. The score matrixSbetween the transformed features is ﬁrst cal- culated byS(i;j) =1 \u001c\u0001h~FA\n",
      "tr(i);~FB tr(j)i. When matching with OT,\u0000Scan be used as the cost matrix of the partial assignment\n",
      "problem as in [37]. We can also apply softmax on both dimensions (referred to as dual-softmax in the\n",
      "fol- lowing) ofSto obtain the probability of soft mutual nearest neighbor matching. Formally, when\n",
      "using dual-softmax, the matching probability Pcis obtained by: Pc(i;j) = softmax (S(i;\u0001))j\u0001softmax\n",
      "(S(\u0001;j))i: Match Selection. Based on the conﬁdence matrix Pc, we select matches with conﬁdence\n",
      "higher than a threshold of \u0012c, and further enforce the mutual nearest neighbor (MNN) criteria, which\n",
      "ﬁlters possible outlier coarse matches. We denote the coarse-level match predictions as: Mc=f\u0000~i;~j\u0001\n",
      "j8\u0000~i;~j\u0001 2MNN (Pc);Pc\u0000~i;~j\u0001 \u0015\u0012cg: 3.4. Coarse-to-Fine Module After establishing coarse matches,\n",
      "these matches are re- ﬁned to the original image resolution with the coarse-to- ﬁne module. Inspired\n",
      "by [50], we use a correlation-based approach for this purpose. For every coarse match (~i;~j), we\n",
      "ﬁrst locate its position (^i;^j)at ﬁne-level feature maps ^FAand^FB, and then crop two sets of local\n",
      "windows of sizew\u0002w. A smaller LoFTR module then transforms the cropped features within each window\n",
      "by Nftimes, yielding two transformed local feature maps ^FA tr(^i)and^FB tr(^j)cen- tered at\n",
      "^iand^j, respectively. Then, we correlate the center vector of ^FA tr(^i)with all vectors in ^FB\n",
      "tr(^j)and thus produce a heatmap that represents the matching probability of each pixel in the\n",
      "neighborhood of ^jwith^i. By computing ex- pectation over the probability distribution, we get the\n",
      "ﬁnal position ^j0with sub-pixel accuracy on IB. Gathering all the matchesf(^i;^j0)gproduces the ﬁnal\n",
      "ﬁne-level matches Mf. 3.5. Supervision The ﬁnal loss consists of the losses for the coarse-level and\n",
      "the ﬁne-level:L=Lc+Lf. Coarse-level Supervision. The loss function for the coarse-level is the\n",
      "negative log-likelihood loss over the con- ﬁdence matrixPcreturned by either the optimal trans- port\n",
      "layer or the dual-softmax operator. We follow Super- Glue [37] to use camera poses and depth maps to\n",
      "compute the ground-truth labels for the conﬁdence matrix during training. We deﬁne the ground-truth\n",
      "coarse matches Mgt cas the mutual nearest neighbors of the two sets of 1/8-resolutiongrids. The\n",
      "distance between two grids is measured by the re-projection distance of their central locations.\n",
      "More de- tails are provided in the supplementary. With the optimal transport layer, we use the same\n",
      "loss formulation as in [37]. When using dual-softmax for matching, we minimize the negative log-\n",
      "likelihood loss over the grids in Mgt c: Lc=\u00001 jMgt cjX (~i;~j)2Mgt clogPc\u0000~i;~j\u0001 : Fine-level\n",
      "Supervision. We use the`2loss for ﬁne-level reﬁnement. Following [50], for each query point ^i, we\n",
      "also measure its uncertainty by calculating the total variance \u001b2(^i)of the corresponding heatmap.\n",
      "The target is to opti- mize the reﬁned position that has low uncertainty, resulting in the ﬁnal\n",
      "weighted loss function: Lf=1 jMfjX (^i;^j0)2M f1 \u001b2(^i)   ^j0\u0000^j0 gt    2; in which ^j0 gtis\n",
      "calculated by warping each ^ifrom ^FA tr(^i)to ^FB tr(^j)with the ground-truth camera pose and\n",
      "depth. We ignore ( ^i,^j0) if the warped location of ^ifalls out of the local window of ^FB\n",
      "tr(^j)when calculatingLf. The gradient is not backpropagated through \u001b2(^i)during training. 3.6.\n",
      "Implementation Details We train the indoor model of LoFTR on the ScanNet [7] dataset and the outdoor\n",
      "model on the MegaDepth [21] fol- lowing [37]. On ScanNet, the model is trained using Adam with an\n",
      "initial learning rate of 1\u000210\u00003and a batch size of 64. It converges after 24 hours of training on 64\n",
      "GTX 1080Ti GPUs. The local feature CNN uses a modiﬁed ver- sion of ResNet-18 [12] as the backbone.\n",
      "The entire model is trained end-to-end with randomly initialized weights. Nc is set to 4 and Nfis\n",
      "1.\u0012cis chosen to 0.2. Window size wis equal to 5. ~FA trand~FB trare upsampled and concate- nated\n",
      "with ^FAand^FBbefore passing through the ﬁne-level LoFTR in the implementation. The full model with\n",
      "dual- softmax matching runs at 116 ms for a 640 \u0002480 image pair on an RTX 2080Ti. Under the optimal\n",
      "transport setup, we use three sinkhorn iterations, and the model runs at 130 ms. We refer readers to\n",
      "the supplementary material for more de- tails of training and timing analyses. 4. Experiments 4.1.\n",
      "Homography Estimation In the ﬁrst experiment, we evaluate LoFTR on the widely adopted HPatches\n",
      "dataset [1] for homography estimation. HPatches contains 52 sequences under signiﬁcant illumina-\n",
      "tion changes and 56 sequences that exhibit large variation in viewpoints. 5\n",
      "==================\n",
      "==================\n",
      "Category MethodHomography est. AUC#matches@3px @5px @10px Detector-basedD2Net [11]+NN 23.2 35.9 53.6\n",
      "0.2K R2D2 [32]+NN 50.6 63.9 76.8 0.5K DISK [47]+NN 52.3 64.9 78.9 1.1K SP [9]+SuperGlue [37] 53.9\n",
      "68.3 81.7 0.6K Detector-freeSparse-NCNet [33] 48.9 54.2 67.1 1.0K DRC-Net [19] 50.6 56.2 68.3 1.0K\n",
      "LoFTR-DS 65.9 75.6 84.6 1.0K Table 1: Homography estimation on HPatches [7]. The AUC of the corner\n",
      "error in percentage is reported. The sufﬁx DS indicates the differentiable matching with dual-\n",
      "softmax. Evaluation protocol. In every test sequence, one reference image is paired with the rest\n",
      "ﬁve images. All images are re- sized with shorter dimensions equal to 480. For each image pair, we\n",
      "extract a set of matches with LoFTR trained on MegaDepth [21]. We use OpenCV to compute the homog-\n",
      "raphy estimation with RANSAC as the robust estimator. To make a fair comparison to methods that\n",
      "produce different numbers of matches, we compute the corner error between the images warped with the\n",
      "estimated ^Hand the ground- truthHas a correctness identiﬁer as in [9]. Following [37], we report\n",
      "the area under the cumulative curve (AUC) of the corner error up to threshold values of 3, 5, and 10\n",
      "pixels, re- spectively. We report the results of LoFTR with a maximum of 1K output matches. Baseline\n",
      "methods. We compare LoFTR with three cate- gories of methods: 1) detector-based local features\n",
      "includ- ing R2D2 [32], D2Net [11], and DISK [47], 2) a detector- based local feature matcher, i.e.,\n",
      "SuperGlue [37] on top of SuperPoint [9] features, and 3) detector-free matchers in- cluding Sparse-\n",
      "NCNet [33] and DRC-Net [19]. For local features, we extract a maximum of 2K features with which we\n",
      "extract mutual nearest neighbors as the ﬁnal matches. For methods directly outputting matches, we\n",
      "restrict a max- imum of 1K matches, same as LoFTR. We use the default hyperparameters in the\n",
      "original implementations for all the baselines. Results. Tab. 1 shows that LoFTR notably outperforms\n",
      "other baselines under all error thresholds by a signiﬁcant margin. Speciﬁcally, the performance gap\n",
      "between LoFTR and other methods increases with a stricter correctness threshold. We attribute the\n",
      "top performance to the larger number of match candidates provided by the detector-free design and\n",
      "the global receptive ﬁeld brought by the Trans- former. Moreover, the coarse-to-ﬁne module also con-\n",
      "tributes to the estimation accuracy by reﬁning matches to a sub-pixel level. 4.2. Relative Pose\n",
      "Estimation Datasets. We use ScanNet [7] and MegaDepth [21] to demonstrate the effectiveness of LoFTR\n",
      "for pose estimationCategory MethodPose estimation AUC @5° @10° @20° Detector-basedORB [35]+GMS [2]\n",
      "5.21 13.65 25.36 D2-Net [11]+NN 5.25 14.53 27.96 ContextDesc [27]+Ratio Test [26] 6.64 15.01 25.75\n",
      "SP [9]+NN 9.43 21.53 36.40 SP [9]+PointCN [52] 11.40 25.47 41.41 SP [9]+OANet [53] 11.76 26.90 43.85\n",
      "SP [9]+SuperGlue [37] 16.16 33.81 51.84 Detector-freeDRC-Net † [19] 7.69 17.93 30.49 LoFTR-OT† 16.88\n",
      "33.62 50.62 LoFTR-OT 21.51 40.39 57.96 LoFTR-DS 22.06 40.8 57.62 Table 2: Evaluation on ScanNet [7]\n",
      "for indoor pose es- timation. The AUC of the pose error in percentage is re- ported. LoFTR improves\n",
      "the state-of-the-art methods by a large margin. †indicates models trained on MegaDepth. The sufﬁxes\n",
      "OT and DS indicate differentiable matching with optimal transport and dual-softmax, respectively.\n",
      "Category MethodPose estimation AUC @5° @10° @20° Detector-based SP [9]+SuperGlue [37] 42.18 61.16\n",
      "75.96 Detector-freeDRC-Net [19] 27.01 42.96 58.31 LoFTR-OT 50.31 67.14 79.93 LoFTR-DS 52.8 69.19\n",
      "81.18 Table 3: Evaluation on MegaDepth [21] for outdoor pose estimation. Matching with LoFTR results\n",
      "in better perfor- mance in the outdoor pose estimation task. in indoor and outdoor scenes,\n",
      "respectively. ScanNet contains 1613 monocular sequences with ground truth poses and depth maps.\n",
      "Following the proce- dure from SuperGlue [37], we sample 230M image pairs for training, with overlap\n",
      "scores between 0.4 and 0.8. We evaluate our method on the 1500 testing pairs from [37]. All images\n",
      "and depth maps are resized to 640\u0002480. This dataset contains image pairs with wide baselines and\n",
      "exten- sive texture-less regions. MegaDepth consists of 1M internet images of 196 differ- ent\n",
      "outdoor scenes. The authors also provide sparse recon- struction from COLMAP [40] and depth maps\n",
      "computed from multi-view stereo. We follow DISK [47] to only use the scenes of “Sacre Coeur” and\n",
      "“St. Peter’s Square” for validation, from which we sample 1500 pairs for a fair com- parison. Images\n",
      "are resized such that their longer dimen- sions are equal to 840 for training and 1200 for\n",
      "validation. The key challenge on MegaDepth is matching under ex- treme viewpoint changes and\n",
      "repetitive patterns. Evaluation protocol. Following [37], we report the AUC of the pose error at\n",
      "thresholds ( 5\u000e;10\u000e;20\u000e), where the pose error is deﬁned as the maximum of angular error in rota-\n",
      "tion and translation. To recover the camera pose, we solve the essential matrix from predicted\n",
      "matches with RANSAC. We don’t compare the matching precisions between LoFTR and other detector-based\n",
      "methods due to the lack of a well- 6\n",
      "==================\n",
      "==================\n",
      "MethodDay Night (0.25m,2°) / (0.5m,5°) / (1.0m,10°) Local Feature Evaluation on Night-time Queries\n",
      "R2D2 [32]+NN - 71.2 / 86.9 / 98.9 LISRD [31]+SP [9]+AdaLam [4] - 73.3 / 86.9 / 97.9 ISRF [29]+NN -\n",
      "69.1 / 87.4 / 98.4 SP [9]+SuperGlue [37] - 73.3 / 88.0 / 98.4 LoFTR-DS - 72.8 / 88.5 /99.0 Full\n",
      "Visual Localization with HLoc SP [9]+SuperGlue [37] 89.8 /96.1 /99.4 77.0 / 90.6 /100.0 LoFTR-OT\n",
      "88.7 / 95.6 / 99.0 78.5 /90.6 / 99.0 Table 4: Visual localization evaluation on the Aachen Day-Night\n",
      "[54] benchmark v1.1 . The evaluation results on both the local feature evaluation track and the full\n",
      "visual localization track are reported. deﬁned metric (e.g., matching score or recall [13, 30]) for\n",
      "detector-free image matching methods. We consider DRC- Net [19] as the state-of-the-art method in\n",
      "detector-free ap- proaches [34, 33]. Results of indoor pose estimation. LoFTR achieves the best\n",
      "performance in pose accuracy compared to all com- petitors (see Tab. 2 and Fig. 5). Pairing LoFTR\n",
      "with opti- mal transport or dual-softmax as the differentiable matching layer achieves comparable\n",
      "performance. Since the released model of DRC-Net yis trained on MegaDepth, we provide the results of\n",
      "LoFTR ytrained on MegaDepth for a fair com- parison. LoFTRyalso outperforms DRC-Net yby a large\n",
      "margin in this evaluation (see Fig. 5), which demonstrates the generalizability of our model across\n",
      "datasets. Results of Outdoor Pose Estimation. As shown in Tab. 3, LoFTR outperforms the detector-\n",
      "free method DRC-Net by 61% at AUC@10°, demonstrating the effectiveness of the Transformer. For\n",
      "SuperGlue, we use the setup from the open-sourced localization toolbox HLoc [36]. LoFTR out-\n",
      "performs SuperGlue by a large margin (13% at AUC@10°), which demonstrates the effectiveness of the\n",
      "detector-free design. Different from indoor scenes, LoFTR-DS performs better than LoFTR-OT on\n",
      "MegaDepth. More qualitative re- sults can be found in Fig. 5. 4.3. Visual Localization Visual\n",
      "Localization. Besides achieving competitive per- formance for relative pose estimation, LoFTR can\n",
      "also ben- eﬁt visual localization, which is the task to estimate the 6- DoF poses of given images\n",
      "with respect to the correspond- ing 3D scene model. We evaluate LoFTR on the Long-Term Visual\n",
      "Localization Benchmark [43] (referred to as Vis- Loc benchmark in the following). It focuses on\n",
      "benchmark- ing visual localization methods under varying conditions, e.g., day-night changes, scene\n",
      "geometry changes, and in- door scenes with plenty of texture-less areas. Thus, the vi- sual\n",
      "localization task relies on highly robust image match- ing methods.MethodDUC1 DUC2 (0.25m,10°) /\n",
      "(0.5m,10°) / (1.0m,10°) ISRF [29] 39.4 / 58.1 / 70.2 41.2 / 61.1 / 69.5 KAPTURE [14]+R2D2 [32] 41.4\n",
      "/ 60.1 / 73.7 47.3 / 67.2 / 73.3 HLoc [36]+SP [9]+SuperGlue [37] 49.0 / 68.7 / 80.8 53.4 / 77.1 /\n",
      "82.4 HLoc [36]+ LoFTR-OT 47.5 / 72.2 /84.8 54.2 / 74.8 / 85.5 Table 5: Visual localization\n",
      "evaluation on the InLoc [41] benchmark . MethodPose estimation AUC @5° @10° @20° 1) replace LoFTR\n",
      "with convolution 14.98 32.04 49.92 2)1/16coarse-resolution + 1/4ﬁne-resolution 16.75 34.82 54.0 3)\n",
      "positional encoding per layer 18.02 35.64 52.77 4) larger model with Nc= 8;Nf= 2 20.87 40.23 57.56\n",
      "Full (Nc= 4;Nf= 1) 20.06 40.8 57.62 Table 6: Ablation study. Five variants of LoFTR are trained and\n",
      "evaluated both on the ScanNet dataset. Evaluation. We evaluate LoFTR on two tracks of VisLoc that\n",
      "consist of several challenges. First, the “visual local- ization for handheld devices” track\n",
      "requires a full localiza- tion pipeline. It benchmarks on two datasets, the Aachen- Day-Night\n",
      "dataset [38, 54] concerning outdoor scenes and the InLoc [41] dataset concerning indoor scenes. We\n",
      "use open-sourced localization pipeline HLoc [36] with the matches extracted by LoFTR. Second, the\n",
      "“local features for long-term localization” track provides a ﬁxed localiza- tion pipeline to\n",
      "evaluate the local feature extractors them- selves and optionally the matchers. This track uses the\n",
      "Aachen v1.1 dataset [54]. We provide the implementation details of testing LoFTR on VisLoc in the\n",
      "supplementary material. Results. We provide evaluation results of LoFTR in Tab. 4 and Tab. 5. We\n",
      "have evaluated LoFTR pairing with either the optimal transport layer or the dual-softmax operator\n",
      "and report the one with better results. LoFTR-DS outperforms all baselines in the local feature\n",
      "challenge track, showing its robustness under day-night changes. Then, for the vi- sual localization\n",
      "for handheld devices track, LoFTR-OT outperforms all published methods on the challenging In- Loc\n",
      "dataset, which contains extensive appearance changes, more texture-less areas, symmetric and\n",
      "repetitive elements. We attribute the prominence to the use of the Transformer and the optimal\n",
      "transport layer, taking advantage of global information and jointly bringing global consensus into\n",
      "the ﬁnal matches. The detector-free design also plays a criti- cal role, preventing the\n",
      "repeatability problem of detector- based methods in low-texture regions. LoFTR-OT performs on par\n",
      "with the state-of-the-art method SuperPoint + Su- perGlue on night queries of the Aachen v1.1\n",
      "dataset and slightly worse on the day queries. 7\n",
      "==================\n",
      "==================\n",
      "IndoorOutdoorSuperPoint + SuperGlueDRC-NetLoFTR Figure 5: Qualitative results. LoFTR is compared to\n",
      "SuperGlue [37] and DRC-Net [19] in indoor and outdoor environ- ments. LoFTR obtains more correct\n",
      "matches and fewer mismatches, successfully coping with low-texture regions and large viewpoint and\n",
      "illumination changes. The red color indicates epipolar error beyond 5\u000210\u00004for indoor scenes and\n",
      "1\u000210\u00004 for outdoor scenes (in the normalized image coordinates). More qualitative results can be\n",
      "found on the project webpage. SelfCross Feature PCA Figure 6: Visualization of self and cross\n",
      "attention weights and the transformed features. In the ﬁrst two examples, the query point from the\n",
      "low-texture region is able to aggregate the surrounding global information ﬂexibly. For instance,\n",
      "the point on the chair is looking at the edge of the chair. In the last two examples, the query\n",
      "point from the distinctive region can also utilize the richer information from other regions. The\n",
      "feature visualization with PCA further shows that LoFTR learns a position-dependent feature\n",
      "representation. 4.4. Understanding LoFTR Ablation Study. To fully understand the different modules\n",
      "in LoFTR, we evaluate ﬁve different variants with results shown in Tab. 6: 1) Replacing the LoFTR\n",
      "module by con- volution with a comparable number of parameters results in a signiﬁcant drop in AUC\n",
      "as expected. 2) Using a smaller version of LoFTR with 1/16and 1/4resolution feature maps at the\n",
      "coarse and ﬁne level, respectively, results in a running time of 104 ms and a degraded pose\n",
      "estimation accuracy. 3) Using DETR-style [3] Transformer architecture which has positional encoding\n",
      "at each layer, leads to a noticeably de- clined result. 4) Increasing the model capacity by doubling\n",
      "the number of LoFTR layers to Nc= 8andNf= 2barely changes the results. We conduct these experiments\n",
      "using the same training and evaluation protocol as indoor pose estimation on ScanNet with an optimal\n",
      "transport layer for matching. Visualizing Attention. We visualize the attention weights in Fig. 6.5.\n",
      "Conclusion This paper presents a novel detector-free matching ap- proach, named LoFTR, that can\n",
      "establish accurate semi- dense matches with Transformers in a coarse-to-ﬁne man- ner. The proposed\n",
      "LoFTR module uses the self and cross attention layers in Transformers to transform the local fea-\n",
      "tures to be context- and position-dependent, which is crucial for LoFTR to obtain high-quality\n",
      "matches on indistinctive regions with low-texture or repetitive patterns. Our exper- iments show\n",
      "that LoFTR achieves state-of-the-art perfor- mances on relative pose estimation and visual\n",
      "localization on multiple datasets. We believe that LoFTR provides a new direction for detector-free\n",
      "methods in local image feature matching and can be extended to more challenging scenar- ios, e.g.,\n",
      "matching images with severe seasonal changes. Acknowledgement. The authors would like to acknowl-\n",
      "edge the support from the National Key Research and Development Program of China (No.\n",
      "2020AAA0108901), NSFC (No. 61806176), and ZJU-SenseTime Joint Lab of 3D Vision. 8\n",
      "==================\n",
      "==================\n",
      "References [1] Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krys- tian Mikolajczyk. HPatches:\n",
      "A benchmark and evaluation of handcrafted and learned local descriptors. In CVPR , 2017. 5 [2]\n",
      "JiaWang Bian, Wen-Yan Lin, Yasuyuki Matsushita, Sai-Kit Yeung, Tan-Dat Nguyen, and Ming-Ming Cheng.\n",
      "GMS: Grid-based motion statistics for fast, ultra-robust feature cor- respondence. In CVPR , 2017. 6\n",
      "[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\n",
      "Sergey Zagoruyko. End-to- end object detection with transformers. In ECCV , 2020. 3, 4, 8 [4] Luca\n",
      "Cavalli, Viktor Larsson, Martin Ralf Oswald, Torsten Sattler, and Marc Pollefeys. Handcrafted\n",
      "Outlier Detection Revisited. In ECCV , 2020. 7 [5] Krzysztof Choromanski, Valerii Likhosherstov,\n",
      "David Do- han, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz\n",
      "Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. ICLR , 2021. 3 [6]\n",
      "Christopher B Choy, JunYoung Gwak, Silvio Savarese, and Manmohan Chandraker. Universal\n",
      "correspondence network. NeurIPS , 2016. 2 [7] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\n",
      "ber, Thomas Funkhouser, and Matthias Nießner. ScanNet: Richly-annotated 3d reconstructions of indoor\n",
      "scenes. In CVPR , 2017. 5, 6 [8] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi- novich. Toward\n",
      "geometric deep slam. arXiv:1707.07410 . 2 [9] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-\n",
      "novich. SuperPoint: Self-supervised interest point detection and description. In CVPRW , 2018. 2, 6,\n",
      "7 [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\n",
      "Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is\n",
      "worth 16x16 words: Trans- formers for image recognition at scale. ICLR , 2021. 3 [11] Mihai Dusmanu,\n",
      "Ignacio Rocco, Tomas Pajdla, Marc Polle- feys, Josef Sivic, Akihiko Torii, and Torsten Sattler.\n",
      "D2-Net: A trainable cnn for joint detection and description of local features. CVPR , 2019. 2, 6\n",
      "[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\n",
      "recognition. In CVPR , 2016. 5 [13] Jared Heinly, Enrique Dunn, and Jan-Michael Frahm. Com- parative\n",
      "evaluation of binary features. In ECCV , 2012. 7 [14] Martin Humenberger, Yohann Cabon, Nicolas\n",
      "Guerin, Julien Morat, J ´erˆome Revaud, Philippe Rerole, No ´e Pion, Cesar de Souza, Vincent Leroy,\n",
      "and Gabriela Csurka. Robust Image Retrieval-based Visual Localization using Kapture.\n",
      "arXiv:2007.13867 . 7 [15] Wei Jiang, Eduard Trulls, Jan Hosang, Andrea Tagliasacchi, and Kwang Moo\n",
      "Yi. COTR: Correspondence Transformer for Matching Across Images, 2021. 2[16] Chaitanya Joshi.\n",
      "Transformers are Graph Neural Networks. https://thegradient.pub/transformers-are-graph- neural-\n",
      "networks/ , 2020. 2 [17] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc ¸ois\n",
      "Fleuret. Transformers are RNNs: Fast autoregres- sive transformers with linear attention. In ICML ,\n",
      "2020. 3, 4 [18] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Re- former: The efﬁcient\n",
      "transformer. ICLR , 2020. 3 [19] Xinghui Li, Kai Han, Shuda Li, and Victor Prisacariu. Dual-\n",
      "resolution correspondence networks. NeurIPS , 2020. 1, 2, 6, 7, 8 [20] Zhaoshuo Li, Xingtong Liu,\n",
      "Francis X Creighton, Russell H Taylor, and Mathias Unberath. Revisiting Stereo Depth Estimation From\n",
      "a Sequence-to-Sequence Perspective with Transformers. arXiv:2011.02910 . 3 [21] Zhengqi Li and Noah\n",
      "Snavely. Megadepth: Learning single- view depth prediction from internet photos. In CVPR , 2018. 5,\n",
      "6 [22] Tsung-Yi Lin, Piotr Doll ´ar, Ross B. Girshick, Kaiming He, Bharath Hariharan, and Serge J.\n",
      "Belongie. Feature Pyramid Networks for Object Detection. CVPR , 2017. 3 [23] Ce Liu, Jenny Yuen, and\n",
      "Antonio Torralba. SIFT Flow: Dense correspondence across scenes and its applications. T- PAMI ,\n",
      "2010. 2 [24] X. Liu, Y . Zheng, B. Killeen, M. Ishii, G. D. Hager, R. H. Taylor, and M. Unberath.\n",
      "Extremely Dense Point Correspon- dences Using a Learned Feature Descriptor. In CVPR , 2020. 2 [25]\n",
      "Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. GIFT: Learning\n",
      "transformation-invariant dense visual descriptors via group cnns. NeurIPS , 2019. 2 [26] David G\n",
      "Lowe. Distinctive image features from scale- invariant keypoints. IJCV , 2004. 2, 6 [27] Zixin Luo,\n",
      "Tianwei Shen, Lei Zhou, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, and Long Quan. ContextDesc: Lo-\n",
      "cal Descriptor Augmentation with Cross-Modality Context. CVPR , 2019. 6 [28] Zixin Luo, Lei Zhou,\n",
      "Xuyang Bai, Hongkai Chen, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, and Long Quan. ASLFeat:\n",
      "Learning local features of accurate shape and lo- calization. In CVPR , 2020. 2 [29] Iaroslav\n",
      "Melekhov, Gabriel J Brostow, Juho Kannala, and Daniyar Turmukhambetov. Image Stylization for Robust\n",
      "Features. arXiv:2008.06959 . 7 [30] Krystian Mikolajczyk and Cordelia Schmid. A performance\n",
      "evaluation of local descriptors. T-PAMI , 2005. 7 [31] R ´emi Pautrat, Viktor Larsson, Martin R\n",
      "Oswald, and Marc Pollefeys. Online Invariance Selection for Local Feature De- scriptors. In ECCV ,\n",
      "2020. 7 [32] Jerome Revaud, Philippe Weinzaepfel, C ´esar De Souza, Noe Pion, Gabriela Csurka,\n",
      "Yohann Cabon, and Martin Humen- berger. R2D2: repeatable and reliable detector and descrip-\n",
      "tor.NeurIPS , 2019. 2, 6, 7 [33] Ignacio Rocco, Relja Arandjelovi ´c, and Josef Sivic. Efﬁcient\n",
      "neighbourhood consensus networks via submanifold sparse convolutions. In ECCV , 2020. 1, 2, 6, 7 9\n",
      "==================\n",
      "==================\n",
      "[34] Ignacio Rocco, Mircea Cimpoi, Relja Arandjelovi ´c, Akihiko Torii, Tomas Pajdla, and Josef\n",
      "Sivic. Neighbourhood con- sensus networks. NeurIPS , 2018. 1, 2, 5, 7 [35] Ethan Rublee, Vincent\n",
      "Rabaud, Kurt Konolige, and Gary Bradski. ORB: An efﬁcient alternative to SIFT or SURF. InICCV ,\n",
      "2011. 2, 6 [36] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse\n",
      "to ﬁne: Robust hierarchical localization at large scale. In CVPR , 2019. 7 [37] Paul-Edouard Sarlin,\n",
      "Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. SuperGlue: Learning feature matching with\n",
      "graph neural networks. In CVPR , 2020. 1, 2, 4, 5, 6, 7, 8 [38] Torsten Sattler, Tobias Weyand,\n",
      "Bastian Leibe, and Leif Kobbelt. Image Retrieval for Image-Based Localization Re- visited. In BMVC ,\n",
      "2012. 7 [39] Tanner Schmidt, Richard Newcombe, and Dieter Fox. Self- supervised visual descriptor\n",
      "learning for dense correspon- dence. RAL, 2016. 2 [40] Johannes L Schonberger and Jan-Michael Frahm.\n",
      "Structure- from-Motion revisited. In CVPR , 2016. 6 [41] Hajime Taira, Masatoshi Okutomi, Torsten\n",
      "Sattler, Mircea Cimpoi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, and Ak- ihiko Torii. InLoc:\n",
      "Indoor visual localization with dense matching and view synthesis. In CVPR , 2018. 7 [42] Yi Tay,\n",
      "Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey. arXiv:2009.06732\n",
      ". 3 [43] Carl Toft, Will Maddern, Akihiko Torii, Lars Hammarstrand, Erik Stenborg, Daniel Safari,\n",
      "Masatoshi Okutomi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, et al. Long-Term Vi- sual Localization\n",
      "Revisited. T-PAMI , 2020. 7 [44] Prune Truong, Martin Danelljan, L. Gool, and R. Timo- fte. Learning\n",
      "Accurate Dense Correspondences and When to Trust Them. ArXiv , abs/2101.01710, 2021. 2 [45] Prune\n",
      "Truong, Martin Danelljan, Luc Van Gool, and Radu Timofte. GOCor: Bringing Globally Optimized\n",
      "Correspon- dence V olumes into Your Neural Network. In NeurIPS , 2020. 2 [46] Prune Truong, Martin\n",
      "Danelljan, and Radu Timofte. GLU- Net: Global-Local Universal Network for dense ﬂow and\n",
      "correspondences. In CVPR , 2020. 2 [47] Michał Tyszkiewicz, Pascal Fua, and Eduard Trulls. DISK:\n",
      "Learning local features with policy gradient. NeurIPS , 2020. 2, 5, 6 [48] Ashish Vaswani, Noam\n",
      "Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\n",
      "Polosukhin. Attention is all you need. NeurIPS , 2017. 2, 3 [49] Huiyu Wang, Yukun Zhu, Bradley\n",
      "Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-Deeplab: Stand- alone axial-attention\n",
      "for panoptic segmentation. In ECCV , 2020. 3 [50] Qianqian Wang, Xiaowei Zhou, Bharath Hariharan,\n",
      "and Noah Snavely. Learning feature descriptors using camera pose supervision. In ECCV , 2020. 5 [51]\n",
      "Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal Fua. LIFT: Learned invariant feature\n",
      "transform. In ECCV , 2016. 2[52] Kwang Moo Yi, Eduard Trulls, Yuki Ono, Vincent Lepetit, Mathieu\n",
      "Salzmann, and Pascal Fua. Learning to ﬁnd good correspondences. In CVPR , 2018. 6 [53] Jiahui Zhang,\n",
      "Dawei Sun, Zixin Luo, Anbang Yao, Lei Zhou, Tianwei Shen, Yurong Chen, Long Quan, and Hongen Liao.\n",
      "Learning Two-View Correspondences and Geometry Using Order-Aware Network. ICCV , 2019. 6 [54] Zichao\n",
      "Zhang, Torsten Sattler, and Davide Scaramuzza. Ref- erence Pose Generation for Long-term Visual\n",
      "Localization via Learned Features and View Synthesis. IJCV , 2020. 7 10\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "for d in data:\n",
    "    print(\"==================\")\n",
    "    printw(d.page_content)\n",
    "    print(\"==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 46 documents\n"
     ]
    }
   ],
   "source": [
    "# Split the data into smaller chunks\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=500)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "print(f\"There are now {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 0}\n",
      "LoFTR: Detector-Free Local Feature Matching with Transformers Jiaming Sun1;2\u0003Zehong Shen1\u0003Yuang\n",
      "Wang1\u0003Hujun Bao1Xiaowei Zhou1y 1Zhejiang University2SenseTime Research Abstract We present a novel\n",
      "method for local image feature matching. Instead of performing image feature detection, description,\n",
      "and matching sequentially, we propose to ﬁrst establish pixel-wise dense matches at a coarse level\n",
      "and later reﬁne the good matches at a ﬁne level. In contrast to dense methods that use a cost volume\n",
      "to search corre- spondences, we use self and cross attention layers in Trans- former to obtain\n",
      "feature descriptors that are conditioned on both images. The global receptive ﬁeld provided by\n",
      "Trans- former enables our method to produce dense matches in low-texture areas, where feature\n",
      "detectors usually strug- gle to produce repeatable interest points. The experiments on indoor and\n",
      "outdoor datasets show that LoFTR outper- forms state-of-the-art methods by a large margin. LoFTR\n",
      "also ranks ﬁrst on two public benchmarks of visual local- ization among the published methods. Code\n",
      "is available at our project page: https://zju3dv.github.io/loftr/ . 1. Introduction Local feature\n",
      "matching between images is the corner- stone of many 3D computer vision tasks, including structure\n",
      "from motion (SfM), simultaneous localization and mapping (SLAM), visual localization, etc. Given two\n",
      "images to be matched, most existing matching methods consist of three\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 0}\n",
      "also ranks ﬁrst on two public benchmarks of visual local- ization among the published methods. Code\n",
      "is available at our project page: https://zju3dv.github.io/loftr/ . 1. Introduction Local feature\n",
      "matching between images is the corner- stone of many 3D computer vision tasks, including structure\n",
      "from motion (SfM), simultaneous localization and mapping (SLAM), visual localization, etc. Given two\n",
      "images to be matched, most existing matching methods consist of three separate phases: feature\n",
      "detection, feature description, and feature matching. In the detection phase, salient points like\n",
      "corners are ﬁrst detected as interest points from each im- age. Local descriptors are then extracted\n",
      "around neigh- borhood regions of these interest points. The feature de- tection and description\n",
      "phases produce two sets of interest points with descriptors, the point-to-point correspondences of\n",
      "which are later found by nearest neighbor search or more sophisticated matching algorithms. The use\n",
      "of a feature detector reduces the search space of matching, and the resulted sparse correspondences\n",
      "are sufﬁ- cient for most tasks, e.g., camera pose estimation. However, a feature detector may fail\n",
      "to extract enough interest points \u0003The ﬁrst three authors contributed equally. The authors are\n",
      "afﬁliated with the State Key Lab of CAD&CG and ZJU-SenseTime Joint Lab of 3D Vision.yCorresponding\n",
      "author: Xiaowei Zhou. Figure 1: Comparison between the proposed method\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 0}\n",
      "The use of a feature detector reduces the search space of matching, and the resulted sparse\n",
      "correspondences are sufﬁ- cient for most tasks, e.g., camera pose estimation. However, a feature\n",
      "detector may fail to extract enough interest points \u0003The ﬁrst three authors contributed equally. The\n",
      "authors are afﬁliated with the State Key Lab of CAD&CG and ZJU-SenseTime Joint Lab of 3D\n",
      "Vision.yCorresponding author: Xiaowei Zhou. Figure 1: Comparison between the proposed method LoFTR\n",
      "and the detector-based method SuperGlue [37]. This example demonstrates that LoFTR is capable of\n",
      "ﬁnd- ing correspondences on the texture-less wall and the ﬂoor with repetitive patterns, where\n",
      "detector-based methods struggle to ﬁnd repeatable interest points.1 that are repeatable between\n",
      "images due to various factors such as poor texture, repetitive patterns, viewpoint change,\n",
      "illumination variation, and motion blur. This issue is espe- cially prominent in indoor\n",
      "environments, where low-texture regions or repetitive patterns sometimes occupy most areas in the\n",
      "ﬁeld of view. Fig. 1 shows an example. Without re- peatable interest points, it is impossible to ﬁnd\n",
      "correct cor- respondences even with perfect descriptors. Several recent works [34, 33, 19] have\n",
      "attempted to rem- edy this problem by establishing pixel-wise dense matches. Matches with high\n",
      "conﬁdence scores can be selected from the dense matches, and thus feature detection is avoided.\n",
      "However, the dense features extracted by convolutional neu-\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 0}\n",
      "in the ﬁeld of view. Fig. 1 shows an example. Without re- peatable interest points, it is impossible\n",
      "to ﬁnd correct cor- respondences even with perfect descriptors. Several recent works [34, 33, 19]\n",
      "have attempted to rem- edy this problem by establishing pixel-wise dense matches. Matches with high\n",
      "conﬁdence scores can be selected from the dense matches, and thus feature detection is avoided.\n",
      "However, the dense features extracted by convolutional neu- ral networks (CNNs) in these works have\n",
      "limited receptive ﬁeld which may not distinguish indistinctive regions. In- stead, humans ﬁnd\n",
      "correspondences in these indistinctive regions not only based on the local neighborhood, but with a\n",
      "larger global context. For example, low-texture regions in 1Only the inlier matches after RANSAC are\n",
      "shown. The green color indicates a match with epipolar error smaller than 5\u000210\u00004(in the nor- malized\n",
      "image coordinates). 1arXiv:2104.00680v1  [cs.CV]  1 Apr 2021\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 1}\n",
      "Fig. 1 can be distinguished according to their relative po- sitions to the edges. This observation\n",
      "tells us that a large receptive ﬁeld in the feature extraction network is crucial. Motivated by the\n",
      "above observations, we propose Lo- cal Feature TRansformer (LoFTR), a novel detector-free approach\n",
      "to local feature matching. Inspired by seminal work SuperGlue [37], we use Transformer [48] with\n",
      "self and cross attention layers to process (transform) the dense local features extracted from the\n",
      "convolutional backbone. Dense matches are ﬁrst extracted between the two sets of transformed\n",
      "features at a low feature resolution ( 1=8of the image dimension). Matches with high conﬁdence are\n",
      "se- lected from these dense matches and later reﬁned to a sub- pixel level with a correlation-based\n",
      "approach. The global receptive ﬁeld and positional encoding of Transformer en- able the transformed\n",
      "feature representations to be context- and position-dependent. By interleaving the self and cross\n",
      "attention layers multiple times, LoFTR learns the densely- arranged globally-consented matching\n",
      "priors exhibited in the ground-truth matches. A linear transformer is also adopted to reduce the\n",
      "computational complexity to a man- ageable level. We evaluate the proposed method on several image\n",
      "matching and camera pose estimation tasks with indoor and outdoor datasets. The experiments show\n",
      "that LoFTR out- performs detector-based and detector-free feature matching\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 1}\n",
      "attention layers multiple times, LoFTR learns the densely- arranged globally-consented matching\n",
      "priors exhibited in the ground-truth matches. A linear transformer is also adopted to reduce the\n",
      "computational complexity to a man- ageable level. We evaluate the proposed method on several image\n",
      "matching and camera pose estimation tasks with indoor and outdoor datasets. The experiments show\n",
      "that LoFTR out- performs detector-based and detector-free feature matching baselines by a large\n",
      "margin. LoFTR also achieves state- of-the-art performance and ranks ﬁrst among the published methods\n",
      "on two public benchmarks of visual localization. Compared to detector-based baseline methods, LoFTR\n",
      "can produce high-quality matches even in indistinctive regions with low-textures, motion blur, or\n",
      "repetitive patterns. 2. Related Work Detector-based Local Feature Matching. Detector-based methods\n",
      "have been the dominant approach for local fea- ture matching. Before the age of deep learning, many\n",
      "renowned works in the traditional hand-crafted local fea- tures have achieved good performances.\n",
      "SIFT [26] and ORB [35] are arguably the most successful hand-crafted local features and are widely\n",
      "adopted in many 3D com- puter vision tasks. The performance on large viewpoint and illumination\n",
      "changes of local features can be signif- icantly improved with learning-based methods. Notably, LIFT\n",
      "[51] and MagicPoint [8] are among the ﬁrst success- ful learning-based local features. They adopt\n",
      "the detector-\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 1}\n",
      "tures have achieved good performances. SIFT [26] and ORB [35] are arguably the most successful hand-\n",
      "crafted local features and are widely adopted in many 3D com- puter vision tasks. The performance on\n",
      "large viewpoint and illumination changes of local features can be signif- icantly improved with\n",
      "learning-based methods. Notably, LIFT [51] and MagicPoint [8] are among the ﬁrst success- ful\n",
      "learning-based local features. They adopt the detector- based design in hand-crafted methods and\n",
      "achieve good performance. SuperPoint [9] builds upon MagicPoint and proposes a self-supervised\n",
      "training method through homo- graphic adaptation. Many learning-based local features along this line\n",
      "[32, 11, 25, 28, 47] also adopt the detector- based design.The above-mentioned local features use\n",
      "the nearest neighbor search to ﬁnd matches between the extracted in- terest points. Recently,\n",
      "SuperGlue [37] proposes a learning- based approach for local feature matching. SuperGlue ac- cepts\n",
      "two sets of interest points with their descriptors as input and learns their matches with a graph\n",
      "neural net- work (GNN), which is a general form of Transformers [16]. Since the priors in feature\n",
      "matching can be learned with a data-driven approach, SuperGlue achieves impressive per- formance and\n",
      "sets the new state of the art in local feature matching. However, being a detector-dependent method,\n",
      "it has the fundamental drawback of being unable to detect repeatable interest points in\n",
      "indistinctive regions. The at-\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 1}\n",
      "input and learns their matches with a graph neural net- work (GNN), which is a general form of\n",
      "Transformers [16]. Since the priors in feature matching can be learned with a data-driven approach,\n",
      "SuperGlue achieves impressive per- formance and sets the new state of the art in local feature\n",
      "matching. However, being a detector-dependent method, it has the fundamental drawback of being\n",
      "unable to detect repeatable interest points in indistinctive regions. The at- tention range in\n",
      "SuperGlue is also limited to the detected interest points only. Our work is inspired by SuperGlue in\n",
      "terms of using self and cross attention in GNN for message passing between two sets of descriptors,\n",
      "but we propose a detector-free design to avoid the drawbacks of feature de- tectors. We also use an\n",
      "efﬁcient variant of the attention lay- ers in Transformer to reduce the computation costs. Detector-\n",
      "free Local Feature Matching. Detector-free methods remove the feature detector phase and directly\n",
      "pro- duce dense descriptors or dense feature matches. The idea of dense features matching dates back\n",
      "to SIFT Flow [23]. [6, 39] are the ﬁrst learning-based approaches to learn pixel- wise feature\n",
      "descriptors with the contrastive loss. Similar to the detector-based methods, the nearest neighbor\n",
      "search is usually used as a post-processing step to match the dense descriptors. NCNet [34] proposed\n",
      "a different approach by directly learning the dense correspondences in an end-to-\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 1}\n",
      "duce dense descriptors or dense feature matches. The idea of dense features matching dates back to\n",
      "SIFT Flow [23]. [6, 39] are the ﬁrst learning-based approaches to learn pixel- wise feature\n",
      "descriptors with the contrastive loss. Similar to the detector-based methods, the nearest neighbor\n",
      "search is usually used as a post-processing step to match the dense descriptors. NCNet [34] proposed\n",
      "a different approach by directly learning the dense correspondences in an end-to- end manner. It\n",
      "constructs 4D cost volumes to enumer- ate all the possible matches between the images and uses 4D\n",
      "convolutions to regularize the cost volume and enforce neighborhood consensus among all the matches.\n",
      "Sparse NCNet [33] improves upon NCNet and makes it more ef- ﬁcient with sparse convolutions.\n",
      "Concurrently with our work, DRC-Net [19] follows this line of work and proposes a coarse-to-ﬁne\n",
      "approach to produce dense matches with higher accuracy. Although all the possible matches are con-\n",
      "sidered in the 4D cost volume, the receptive ﬁeld of 4D con- volution is still limited to each\n",
      "matches’ neighborhood area. Apart from neighborhood consensus, our work focuses on achieving global\n",
      "consensus between matches with the help of the global receptive ﬁeld in Transformers, which is not\n",
      "exploited in NCNet and its follow-up works. [24] proposes a dense matching pipeline for SfM with\n",
      "endoscopy videos. The recent line of research [46, 45, 44, 15] that focuses on\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 1}\n",
      "sidered in the 4D cost volume, the receptive ﬁeld of 4D con- volution is still limited to each\n",
      "matches’ neighborhood area. Apart from neighborhood consensus, our work focuses on achieving global\n",
      "consensus between matches with the help of the global receptive ﬁeld in Transformers, which is not\n",
      "exploited in NCNet and its follow-up works. [24] proposes a dense matching pipeline for SfM with\n",
      "endoscopy videos. The recent line of research [46, 45, 44, 15] that focuses on bridging the task of\n",
      "local feature matching and optical ﬂow estimation, is also related to our work. Transformers in\n",
      "Vision Related Tasks. Transformer [48] has become the de facto standard for sequence modeling in\n",
      "natural language processing (NLP) due to their simplic- 2\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 2}\n",
      "1. Local Feature CNN IA,IB1/8 1 Self-Attention LayerCross-Attention Layer conﬁdence matrix ˆFA,ˆFB\n",
      "ﬂatten E(·) expectation 4. Coarse-to-Fine Modulecorrelation& softmax w⇥w cropping onˆF ˆiˆjˆj0\n",
      "(˜i,˜j) LoFTR Module(1/8)2HBWB (1/8)2HAWA LoFTRModule3. Matching Module 2. Coarse-Level Local\n",
      "Feature Transform˜FAtr,˜FBtr ˜FA,˜FB Matching LayerDi↵erentiable positionalencoding⇥Nc⇥Nf for every\n",
      "coarse prediction (˜i,˜j)2McPc Mf={(ˆi,ˆj0)}1/2Figure 2: Overview of the proposed method. LoFTR has\n",
      "four components: 1.A local feature CNN extracts the coarse-level feature maps ~FAand~FB, together\n",
      "with the ﬁne-level feature maps ^FAand^FBfrom the image pair IAandIB(Section 3.1). 2.The coarse\n",
      "feature maps are ﬂattened to 1-D vectors and added with the positional encoding. The added features\n",
      "are then processed by the Local Feature TRansformer (LoFTR) module, which has Ncself-attention and\n",
      "cross-attention layers (Section 3.2). 3.A differentiable matching layer is used to match the\n",
      "transformed features, which ends up with a conﬁdence matrix Pc. The matches inPcare selected\n",
      "according to the conﬁdence threshold and mutual-nearest-neighbor criteria, yielding the coarse-level\n",
      "match prediction Mc(Section 3.3). 4.For every selected coarse prediction (~i;~j)2Mc, a local window\n",
      "with size w\u0002wis cropped from the ﬁne-level feature map. Coarse matches will be reﬁned within this\n",
      "local window to a sub-pixel level as the ﬁnal match prediction Mf(Section 3.4).\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 2}\n",
      "conﬁdence matrix Pc. The matches inPcare selected according to the conﬁdence threshold and mutual-\n",
      "nearest-neighbor criteria, yielding the coarse-level match prediction Mc(Section 3.3). 4.For every\n",
      "selected coarse prediction (~i;~j)2Mc, a local window with size w\u0002wis cropped from the ﬁne-level\n",
      "feature map. Coarse matches will be reﬁned within this local window to a sub-pixel level as the ﬁnal\n",
      "match prediction Mf(Section 3.4). ity and computation efﬁciency. Recently, Transformers are also\n",
      "getting more attention in computer vision tasks, such as image classiﬁcation [10], object detection\n",
      "[3] and seman- tic segmentation [49]. Concurrently with our work, [20] proposes to use Transformer\n",
      "for disparity estimation. The computation cost of the vanilla Transformer grows quadrat- ically as\n",
      "the length of input sequences due to the multipli- cation between query and key vectors. Many\n",
      "efﬁcient vari- ants [42, 18, 17, 5] are proposed recently in the context of processing long language\n",
      "sequences. Since no assumption of the input data is made in these works, they are also well suited\n",
      "for processing images. 3. Methods Given the image pair IAandIB, the existing local fea- ture\n",
      "matching methods use a feature detector to extract in- terest points. We propose to tackle the\n",
      "repeatability issue of feature detectors with a detector-free design. An overview of the proposed\n",
      "method LoFTR is presented in Fig. 2. 3.1. Local Feature Extraction We use a standard convolutional\n",
      "architecture with\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 2}\n",
      "of the input data is made in these works, they are also well suited for processing images. 3.\n",
      "Methods Given the image pair IAandIB, the existing local fea- ture matching methods use a feature\n",
      "detector to extract in- terest points. We propose to tackle the repeatability issue of feature\n",
      "detectors with a detector-free design. An overview of the proposed method LoFTR is presented in Fig.\n",
      "2. 3.1. Local Feature Extraction We use a standard convolutional architecture with FPN [22] (denoted\n",
      "as the local feature CNN) to extractmulti-level features from both images. We use ~FAand~FB to\n",
      "denote the coarse-level features at 1=8of the original im- age dimension, and ^FAand^FBthe ﬁne-level\n",
      "features at 1=2 of the original image dimension. Convolutional Neural Networks (CNNs) possess the\n",
      "in- ductive bias of translation equivariance and locality, which are well suited for local feature\n",
      "extraction. The downsam- pling introduced by the CNN also reduces the input length of the LoFTR\n",
      "module, which is crucial to ensure a manage- able computation cost. 3.2. Local Feature Transformer\n",
      "(LoFTR) Module After the local feature extraction, ~FAand~FBare passed through the LoFTR module to\n",
      "extract position and context dependent local features. Intuitively, the LoFTR module transforms the\n",
      "features into feature representations that are easy to match. We denote the transformed features as\n",
      "~FA tr and~FB tr. Preliminaries: Transformer [48]. We ﬁrst brieﬂy intro-\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 2}\n",
      "of the LoFTR module, which is crucial to ensure a manage- able computation cost. 3.2. Local Feature\n",
      "Transformer (LoFTR) Module After the local feature extraction, ~FAand~FBare passed through the LoFTR\n",
      "module to extract position and context dependent local features. Intuitively, the LoFTR module\n",
      "transforms the features into feature representations that are easy to match. We denote the\n",
      "transformed features as ~FA tr and~FB tr. Preliminaries: Transformer [48]. We ﬁrst brieﬂy intro-\n",
      "duce the Transformer here as background. A Transformer encoder is composed of sequentially connected\n",
      "encoder lay- ers. Fig. 3(a) shows the architecture of an encoder layer. The key element in the\n",
      "encoder layer is the attention 3\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 3}\n",
      "QKV QKV elu(·)+1elu(·)+1 (a)(b) (c) Q=WQfiK=WKfjV=WVfjh ConcatMatMul MatMul MatMulMatMul Softmax\n",
      "Dot-Product AttentionLinear Attention fifj fi Attention LayerDk⇥Dv Nq⇥Nk Cat&Linear&Norm Feed-\n",
      "Forward&NormAddFigure 3: Encoder layer and attention layer in LoFTR. (a) Transformer encoder layer.\n",
      "hrepresents the multiple heads of attention. (b) Vanilla dot-product attention with O(N2)complexity.\n",
      "(c) Linear attention layer with O(N) complexity. The scale factor is omitted for simplicity. layer.\n",
      "The input vectors for an attention layer are conven- tionally named query, key, and value. Analogous\n",
      "to infor- mation retrieval, the query vector Qretrieves information from the value vector V,\n",
      "according to the attention weight computed from the dot product of Qand the key vector K\n",
      "corresponding to each value V. The computation graph of the attention layer is presented in Fig.\n",
      "3(b). Formally, the attention layer is denoted as: Attention (Q;K;V ) = softmax( QKT)V: Intuitively,\n",
      "the attention operation selects the relevant infor- mation by measuring the similarity between the\n",
      "query ele- ment and each key element. The output vector is the sum of the value vectors weighted by\n",
      "the similarity scores. As a result, the relevant information is extracted from the value vector if\n",
      "the similarity is high. This process is also called “message passing” in Graph Neural Network.\n",
      "Linear Transformer. Denoting the length of QandKas Nand their feature dimension as D, the dot\n",
      "product be-\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 3}\n",
      "mation by measuring the similarity between the query ele- ment and each key element. The output\n",
      "vector is the sum of the value vectors weighted by the similarity scores. As a result, the relevant\n",
      "information is extracted from the value vector if the similarity is high. This process is also\n",
      "called “message passing” in Graph Neural Network. Linear Transformer. Denoting the length of QandKas\n",
      "Nand their feature dimension as D, the dot product be- tweenQandKin the Transformer introduces\n",
      "computation cost that grows quadratically ( O(N2)) with the length of the input sequence. Directly\n",
      "applying the vanilla version of Transformer in the context of local feature matching is im-\n",
      "practical even when the input length is reduced by the local feature CNN. To remedy this problem, we\n",
      "propose to use an efﬁcient variant of the vanilla attention layer in Trans- former. Linear\n",
      "Transformer [17] proposes to reduce the computation complexity of Transformer to O(N)by sub-\n",
      "stituting the exponential kernel used in the original atten- tion layer with an alternative kernel\n",
      "function sim(Q;K ) = \u001e(Q)\u0001\u001e(K)T;where\u001e(\u0001) = elu(\u0001) + 1 . This operation is illustrated by the\n",
      "computation graph in Fig. 3(c). Utilizing SelfAttentionCrossAttentionFeature VisualizationLRConv\n",
      "#1Conv #2Conv #3 Transformer #1LRATTENTION(b)(a) (c)Figure 4: Illustration of the receptive ﬁeld of\n",
      "(a)Convolu- tions and (b)Transformers. Assume that the objective is to establish a connection\n",
      "between the L and R elements to\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 3}\n",
      "tion layer with an alternative kernel function sim(Q;K ) = \u001e(Q)\u0001\u001e(K)T;where\u001e(\u0001) = elu(\u0001) + 1 . This\n",
      "operation is illustrated by the computation graph in Fig. 3(c). Utilizing\n",
      "SelfAttentionCrossAttentionFeature VisualizationLRConv #1Conv #2Conv #3 Transformer\n",
      "#1LRATTENTION(b)(a) (c)Figure 4: Illustration of the receptive ﬁeld of (a)Convolu- tions and\n",
      "(b)Transformers. Assume that the objective is to establish a connection between the L and R elements\n",
      "to extract their joint feature representation. Due to the local- connectivity of convolutions, many\n",
      "convolution layers need to be stacked together in order to achieve this connection. The global\n",
      "receptive ﬁeld of Transformers enables this con- nection to be established through only one\n",
      "attention layer. (c)Visualization of the attention weights and transformed dense features. We use\n",
      "PCA to reduce the dimension of the transformed features ~FA trand~FB trand visualize the results\n",
      "with RGB color. Zoom in for details. the associativity property of matrix products, the multipli-\n",
      "cation between \u001e(K)TandVcan be carried out ﬁrst. Since D\u001cN, the computation cost is reduced to O(N).\n",
      "Positional Encoding. We use the 2D extension of the standard positional encoding in Transformers\n",
      "following DETR [3]. Different from DETR, we only add them to the backbone output once. We leave the\n",
      "formal deﬁnition of the positional encoding in the supplementary material. In- tuitively, the\n",
      "positional encoding gives each element unique\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 3}\n",
      "the associativity property of matrix products, the multipli- cation between \u001e(K)TandVcan be carried\n",
      "out ﬁrst. Since D\u001cN, the computation cost is reduced to O(N). Positional Encoding. We use the 2D\n",
      "extension of the standard positional encoding in Transformers following DETR [3]. Different from\n",
      "DETR, we only add them to the backbone output once. We leave the formal deﬁnition of the positional\n",
      "encoding in the supplementary material. In- tuitively, the positional encoding gives each element\n",
      "unique position information in the sinusoidal format. By adding the position encoding to ~FAand~FB,\n",
      "the transformed fea- tures will become position-dependent, which is crucial to the ability of LoFTR\n",
      "to produce matches in indistinctive re- gions. As shown in the bottom row of Fig. 4(c), although the\n",
      "input RGB color is homogeneous on the white walls, the transformed features ~FA trand~FB trareunique\n",
      "for each position demonstrated by the smooth color gradients. More visualizations are provided in\n",
      "Fig. 6. Self-attention and Cross-attention Layers. For self- attention layers, the input features\n",
      "fiandfj(shown in Fig. 3) are the same (either ~FAor~FB). For cross-attention layers, the input\n",
      "features fiandfjare either ( ~FAand ~FB) or ( ~FBand~FA) depending on the direction of cross-\n",
      "attention. Following [37], we interleave the self and cross attention layers in the LoFTR module by\n",
      "Nctimes. The attention weights of the self and cross attention layers in\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 3}\n",
      "visualizations are provided in Fig. 6. Self-attention and Cross-attention Layers. For self-\n",
      "attention layers, the input features fiandfj(shown in Fig. 3) are the same (either ~FAor~FB). For\n",
      "cross-attention layers, the input features fiandfjare either ( ~FAand ~FB) or ( ~FBand~FA) depending\n",
      "on the direction of cross- attention. Following [37], we interleave the self and cross attention\n",
      "layers in the LoFTR module by Nctimes. The attention weights of the self and cross attention layers\n",
      "in LoFTR are visualized in the ﬁrst two rows of Fig. 4(c). 4\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 4}\n",
      "3.3. Establishing Coarse-level Matches Two types of differentiable matching layers can be ap- plied\n",
      "in LoFTR, either with an optimal transport (OT) layer as in [37] or with a dual-softmax operator\n",
      "[34, 47]. The score matrixSbetween the transformed features is ﬁrst cal- culated byS(i;j) =1 \u001c\u0001h~FA\n",
      "tr(i);~FB tr(j)i. When matching with OT,\u0000Scan be used as the cost matrix of the partial assignment\n",
      "problem as in [37]. We can also apply softmax on both dimensions (referred to as dual-softmax in the\n",
      "fol- lowing) ofSto obtain the probability of soft mutual nearest neighbor matching. Formally, when\n",
      "using dual-softmax, the matching probability Pcis obtained by: Pc(i;j) = softmax (S(i;\u0001))j\u0001softmax\n",
      "(S(\u0001;j))i: Match Selection. Based on the conﬁdence matrix Pc, we select matches with conﬁdence\n",
      "higher than a threshold of \u0012c, and further enforce the mutual nearest neighbor (MNN) criteria, which\n",
      "ﬁlters possible outlier coarse matches. We denote the coarse-level match predictions as: Mc=f\u0000~i;~j\u0001\n",
      "j8\u0000~i;~j\u0001 2MNN (Pc);Pc\u0000~i;~j\u0001 \u0015\u0012cg: 3.4. Coarse-to-Fine Module After establishing coarse matches,\n",
      "these matches are re- ﬁned to the original image resolution with the coarse-to- ﬁne module. Inspired\n",
      "by [50], we use a correlation-based approach for this purpose. For every coarse match (~i;~j), we\n",
      "ﬁrst locate its position (^i;^j)at ﬁne-level feature maps ^FAand^FB, and then crop two sets of local\n",
      "windows of sizew\u0002w. A smaller LoFTR module then transforms the\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 4}\n",
      "Mc=f\u0000~i;~j\u0001 j8\u0000~i;~j\u0001 2MNN (Pc);Pc\u0000~i;~j\u0001 \u0015\u0012cg: 3.4. Coarse-to-Fine Module After establishing coarse\n",
      "matches, these matches are re- ﬁned to the original image resolution with the coarse-to- ﬁne module.\n",
      "Inspired by [50], we use a correlation-based approach for this purpose. For every coarse match\n",
      "(~i;~j), we ﬁrst locate its position (^i;^j)at ﬁne-level feature maps ^FAand^FB, and then crop two\n",
      "sets of local windows of sizew\u0002w. A smaller LoFTR module then transforms the cropped features within\n",
      "each window by Nftimes, yielding two transformed local feature maps ^FA tr(^i)and^FB tr(^j)cen-\n",
      "tered at ^iand^j, respectively. Then, we correlate the center vector of ^FA tr(^i)with all vectors\n",
      "in ^FB tr(^j)and thus produce a heatmap that represents the matching probability of each pixel in\n",
      "the neighborhood of ^jwith^i. By computing ex- pectation over the probability distribution, we get\n",
      "the ﬁnal position ^j0with sub-pixel accuracy on IB. Gathering all the matchesf(^i;^j0)gproduces the\n",
      "ﬁnal ﬁne-level matches Mf. 3.5. Supervision The ﬁnal loss consists of the losses for the coarse-\n",
      "level and the ﬁne-level:L=Lc+Lf. Coarse-level Supervision. The loss function for the coarse-level is\n",
      "the negative log-likelihood loss over the con- ﬁdence matrixPcreturned by either the optimal trans-\n",
      "port layer or the dual-softmax operator. We follow Super- Glue [37] to use camera poses and depth\n",
      "maps to compute the ground-truth labels for the conﬁdence matrix during\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 4}\n",
      "matchesf(^i;^j0)gproduces the ﬁnal ﬁne-level matches Mf. 3.5. Supervision The ﬁnal loss consists of\n",
      "the losses for the coarse-level and the ﬁne-level:L=Lc+Lf. Coarse-level Supervision. The loss\n",
      "function for the coarse-level is the negative log-likelihood loss over the con- ﬁdence\n",
      "matrixPcreturned by either the optimal trans- port layer or the dual-softmax operator. We follow\n",
      "Super- Glue [37] to use camera poses and depth maps to compute the ground-truth labels for the\n",
      "conﬁdence matrix during training. We deﬁne the ground-truth coarse matches Mgt cas the mutual\n",
      "nearest neighbors of the two sets of 1/8-resolutiongrids. The distance between two grids is measured\n",
      "by the re-projection distance of their central locations. More de- tails are provided in the\n",
      "supplementary. With the optimal transport layer, we use the same loss formulation as in [37]. When\n",
      "using dual-softmax for matching, we minimize the negative log-likelihood loss over the grids in Mgt\n",
      "c: Lc=\u00001 jMgt cjX (~i;~j)2Mgt clogPc\u0000~i;~j\u0001 : Fine-level Supervision. We use the`2loss for ﬁne-level\n",
      "reﬁnement. Following [50], for each query point ^i, we also measure its uncertainty by calculating\n",
      "the total variance \u001b2(^i)of the corresponding heatmap. The target is to opti- mize the reﬁned\n",
      "position that has low uncertainty, resulting in the ﬁnal weighted loss function: Lf=1 jMfjX\n",
      "(^i;^j0)2M f1 \u001b2(^i)   ^j0\u0000^j0 gt    2; in which ^j0 gtis calculated by warping each ^ifrom ^FA\n",
      "tr(^i)to ^FB\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 4}\n",
      "c: Lc=\u00001 jMgt cjX (~i;~j)2Mgt clogPc\u0000~i;~j\u0001 : Fine-level Supervision. We use the`2loss for ﬁne-level\n",
      "reﬁnement. Following [50], for each query point ^i, we also measure its uncertainty by calculating\n",
      "the total variance \u001b2(^i)of the corresponding heatmap. The target is to opti- mize the reﬁned\n",
      "position that has low uncertainty, resulting in the ﬁnal weighted loss function: Lf=1 jMfjX\n",
      "(^i;^j0)2M f1 \u001b2(^i)   ^j0\u0000^j0 gt    2; in which ^j0 gtis calculated by warping each ^ifrom ^FA\n",
      "tr(^i)to ^FB tr(^j)with the ground-truth camera pose and depth. We ignore ( ^i,^j0) if the warped\n",
      "location of ^ifalls out of the local window of ^FB tr(^j)when calculatingLf. The gradient is not\n",
      "backpropagated through \u001b2(^i)during training. 3.6. Implementation Details We train the indoor model\n",
      "of LoFTR on the ScanNet [7] dataset and the outdoor model on the MegaDepth [21] fol- lowing [37]. On\n",
      "ScanNet, the model is trained using Adam with an initial learning rate of 1\u000210\u00003and a batch size of\n",
      "64. It converges after 24 hours of training on 64 GTX 1080Ti GPUs. The local feature CNN uses a\n",
      "modiﬁed ver- sion of ResNet-18 [12] as the backbone. The entire model is trained end-to-end with\n",
      "randomly initialized weights. Nc is set to 4 and Nfis 1.\u0012cis chosen to 0.2. Window size wis equal to\n",
      "5. ~FA trand~FB trare upsampled and concate- nated with ^FAand^FBbefore passing through the ﬁne-\n",
      "level LoFTR in the implementation. The full model with dual- softmax matching runs at 116 ms for a\n",
      "640 \u0002480 image pair\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 4}\n",
      "1080Ti GPUs. The local feature CNN uses a modiﬁed ver- sion of ResNet-18 [12] as the backbone. The\n",
      "entire model is trained end-to-end with randomly initialized weights. Nc is set to 4 and Nfis 1.\u0012cis\n",
      "chosen to 0.2. Window size wis equal to 5. ~FA trand~FB trare upsampled and concate- nated with\n",
      "^FAand^FBbefore passing through the ﬁne-level LoFTR in the implementation. The full model with dual-\n",
      "softmax matching runs at 116 ms for a 640 \u0002480 image pair on an RTX 2080Ti. Under the optimal\n",
      "transport setup, we use three sinkhorn iterations, and the model runs at 130 ms. We refer readers to\n",
      "the supplementary material for more de- tails of training and timing analyses. 4. Experiments 4.1.\n",
      "Homography Estimation In the ﬁrst experiment, we evaluate LoFTR on the widely adopted HPatches\n",
      "dataset [1] for homography estimation. HPatches contains 52 sequences under signiﬁcant illumina-\n",
      "tion changes and 56 sequences that exhibit large variation in viewpoints. 5\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 5}\n",
      "Category MethodHomography est. AUC#matches@3px @5px @10px Detector-basedD2Net [11]+NN 23.2 35.9 53.6\n",
      "0.2K R2D2 [32]+NN 50.6 63.9 76.8 0.5K DISK [47]+NN 52.3 64.9 78.9 1.1K SP [9]+SuperGlue [37] 53.9\n",
      "68.3 81.7 0.6K Detector-freeSparse-NCNet [33] 48.9 54.2 67.1 1.0K DRC-Net [19] 50.6 56.2 68.3 1.0K\n",
      "LoFTR-DS 65.9 75.6 84.6 1.0K Table 1: Homography estimation on HPatches [7]. The AUC of the corner\n",
      "error in percentage is reported. The sufﬁx DS indicates the differentiable matching with dual-\n",
      "softmax. Evaluation protocol. In every test sequence, one reference image is paired with the rest\n",
      "ﬁve images. All images are re- sized with shorter dimensions equal to 480. For each image pair, we\n",
      "extract a set of matches with LoFTR trained on MegaDepth [21]. We use OpenCV to compute the homog-\n",
      "raphy estimation with RANSAC as the robust estimator. To make a fair comparison to methods that\n",
      "produce different numbers of matches, we compute the corner error between the images warped with the\n",
      "estimated ^Hand the ground- truthHas a correctness identiﬁer as in [9]. Following [37], we report\n",
      "the area under the cumulative curve (AUC) of the corner error up to threshold values of 3, 5, and 10\n",
      "pixels, re- spectively. We report the results of LoFTR with a maximum of 1K output matches. Baseline\n",
      "methods. We compare LoFTR with three cate- gories of methods: 1) detector-based local features\n",
      "includ- ing R2D2 [32], D2Net [11], and DISK [47], 2) a detector-\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 5}\n",
      "the images warped with the estimated ^Hand the ground- truthHas a correctness identiﬁer as in [9].\n",
      "Following [37], we report the area under the cumulative curve (AUC) of the corner error up to\n",
      "threshold values of 3, 5, and 10 pixels, re- spectively. We report the results of LoFTR with a\n",
      "maximum of 1K output matches. Baseline methods. We compare LoFTR with three cate- gories of methods:\n",
      "1) detector-based local features includ- ing R2D2 [32], D2Net [11], and DISK [47], 2) a detector-\n",
      "based local feature matcher, i.e., SuperGlue [37] on top of SuperPoint [9] features, and 3)\n",
      "detector-free matchers in- cluding Sparse-NCNet [33] and DRC-Net [19]. For local features, we\n",
      "extract a maximum of 2K features with which we extract mutual nearest neighbors as the ﬁnal matches.\n",
      "For methods directly outputting matches, we restrict a max- imum of 1K matches, same as LoFTR. We\n",
      "use the default hyperparameters in the original implementations for all the baselines. Results. Tab.\n",
      "1 shows that LoFTR notably outperforms other baselines under all error thresholds by a signiﬁcant\n",
      "margin. Speciﬁcally, the performance gap between LoFTR and other methods increases with a stricter\n",
      "correctness threshold. We attribute the top performance to the larger number of match candidates\n",
      "provided by the detector-free design and the global receptive ﬁeld brought by the Trans- former.\n",
      "Moreover, the coarse-to-ﬁne module also con- tributes to the estimation accuracy by reﬁning matches\n",
      "to a sub-pixel level.\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 5}\n",
      "other baselines under all error thresholds by a signiﬁcant margin. Speciﬁcally, the performance gap\n",
      "between LoFTR and other methods increases with a stricter correctness threshold. We attribute the\n",
      "top performance to the larger number of match candidates provided by the detector-free design and\n",
      "the global receptive ﬁeld brought by the Trans- former. Moreover, the coarse-to-ﬁne module also con-\n",
      "tributes to the estimation accuracy by reﬁning matches to a sub-pixel level. 4.2. Relative Pose\n",
      "Estimation Datasets. We use ScanNet [7] and MegaDepth [21] to demonstrate the effectiveness of LoFTR\n",
      "for pose estimationCategory MethodPose estimation AUC @5° @10° @20° Detector-basedORB [35]+GMS [2]\n",
      "5.21 13.65 25.36 D2-Net [11]+NN 5.25 14.53 27.96 ContextDesc [27]+Ratio Test [26] 6.64 15.01 25.75\n",
      "SP [9]+NN 9.43 21.53 36.40 SP [9]+PointCN [52] 11.40 25.47 41.41 SP [9]+OANet [53] 11.76 26.90 43.85\n",
      "SP [9]+SuperGlue [37] 16.16 33.81 51.84 Detector-freeDRC-Net † [19] 7.69 17.93 30.49 LoFTR-OT† 16.88\n",
      "33.62 50.62 LoFTR-OT 21.51 40.39 57.96 LoFTR-DS 22.06 40.8 57.62 Table 2: Evaluation on ScanNet [7]\n",
      "for indoor pose es- timation. The AUC of the pose error in percentage is re- ported. LoFTR improves\n",
      "the state-of-the-art methods by a large margin. †indicates models trained on MegaDepth. The sufﬁxes\n",
      "OT and DS indicate differentiable matching with optimal transport and dual-softmax, respectively.\n",
      "Category MethodPose estimation AUC @5° @10° @20° Detector-based SP [9]+SuperGlue [37] 42.18 61.16\n",
      "75.96\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 5}\n",
      "LoFTR-OT 21.51 40.39 57.96 LoFTR-DS 22.06 40.8 57.62 Table 2: Evaluation on ScanNet [7] for indoor\n",
      "pose es- timation. The AUC of the pose error in percentage is re- ported. LoFTR improves the state-\n",
      "of-the-art methods by a large margin. †indicates models trained on MegaDepth. The sufﬁxes OT and DS\n",
      "indicate differentiable matching with optimal transport and dual-softmax, respectively. Category\n",
      "MethodPose estimation AUC @5° @10° @20° Detector-based SP [9]+SuperGlue [37] 42.18 61.16 75.96\n",
      "Detector-freeDRC-Net [19] 27.01 42.96 58.31 LoFTR-OT 50.31 67.14 79.93 LoFTR-DS 52.8 69.19 81.18\n",
      "Table 3: Evaluation on MegaDepth [21] for outdoor pose estimation. Matching with LoFTR results in\n",
      "better perfor- mance in the outdoor pose estimation task. in indoor and outdoor scenes,\n",
      "respectively. ScanNet contains 1613 monocular sequences with ground truth poses and depth maps.\n",
      "Following the proce- dure from SuperGlue [37], we sample 230M image pairs for training, with overlap\n",
      "scores between 0.4 and 0.8. We evaluate our method on the 1500 testing pairs from [37]. All images\n",
      "and depth maps are resized to 640\u0002480. This dataset contains image pairs with wide baselines and\n",
      "exten- sive texture-less regions. MegaDepth consists of 1M internet images of 196 differ- ent\n",
      "outdoor scenes. The authors also provide sparse recon- struction from COLMAP [40] and depth maps\n",
      "computed from multi-view stereo. We follow DISK [47] to only use the scenes of “Sacre Coeur” and\n",
      "“St. Peter’s Square” for\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 5}\n",
      "evaluate our method on the 1500 testing pairs from [37]. All images and depth maps are resized to\n",
      "640\u0002480. This dataset contains image pairs with wide baselines and exten- sive texture-less regions.\n",
      "MegaDepth consists of 1M internet images of 196 differ- ent outdoor scenes. The authors also provide\n",
      "sparse recon- struction from COLMAP [40] and depth maps computed from multi-view stereo. We follow\n",
      "DISK [47] to only use the scenes of “Sacre Coeur” and “St. Peter’s Square” for validation, from\n",
      "which we sample 1500 pairs for a fair com- parison. Images are resized such that their longer dimen-\n",
      "sions are equal to 840 for training and 1200 for validation. The key challenge on MegaDepth is\n",
      "matching under ex- treme viewpoint changes and repetitive patterns. Evaluation protocol. Following\n",
      "[37], we report the AUC of the pose error at thresholds ( 5\u000e;10\u000e;20\u000e), where the pose error is\n",
      "deﬁned as the maximum of angular error in rota- tion and translation. To recover the camera pose, we\n",
      "solve the essential matrix from predicted matches with RANSAC. We don’t compare the matching\n",
      "precisions between LoFTR and other detector-based methods due to the lack of a well- 6\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 6}\n",
      "MethodDay Night (0.25m,2°) / (0.5m,5°) / (1.0m,10°) Local Feature Evaluation on Night-time Queries\n",
      "R2D2 [32]+NN - 71.2 / 86.9 / 98.9 LISRD [31]+SP [9]+AdaLam [4] - 73.3 / 86.9 / 97.9 ISRF [29]+NN -\n",
      "69.1 / 87.4 / 98.4 SP [9]+SuperGlue [37] - 73.3 / 88.0 / 98.4 LoFTR-DS - 72.8 / 88.5 /99.0 Full\n",
      "Visual Localization with HLoc SP [9]+SuperGlue [37] 89.8 /96.1 /99.4 77.0 / 90.6 /100.0 LoFTR-OT\n",
      "88.7 / 95.6 / 99.0 78.5 /90.6 / 99.0 Table 4: Visual localization evaluation on the Aachen Day-Night\n",
      "[54] benchmark v1.1 . The evaluation results on both the local feature evaluation track and the full\n",
      "visual localization track are reported. deﬁned metric (e.g., matching score or recall [13, 30]) for\n",
      "detector-free image matching methods. We consider DRC- Net [19] as the state-of-the-art method in\n",
      "detector-free ap- proaches [34, 33]. Results of indoor pose estimation. LoFTR achieves the best\n",
      "performance in pose accuracy compared to all com- petitors (see Tab. 2 and Fig. 5). Pairing LoFTR\n",
      "with opti- mal transport or dual-softmax as the differentiable matching layer achieves comparable\n",
      "performance. Since the released model of DRC-Net yis trained on MegaDepth, we provide the results of\n",
      "LoFTR ytrained on MegaDepth for a fair com- parison. LoFTRyalso outperforms DRC-Net yby a large\n",
      "margin in this evaluation (see Fig. 5), which demonstrates the generalizability of our model across\n",
      "datasets. Results of Outdoor Pose Estimation. As shown in Tab. 3, LoFTR outperforms the detector-\n",
      "free method DRC-Net by\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 6}\n",
      "layer achieves comparable performance. Since the released model of DRC-Net yis trained on MegaDepth,\n",
      "we provide the results of LoFTR ytrained on MegaDepth for a fair com- parison. LoFTRyalso\n",
      "outperforms DRC-Net yby a large margin in this evaluation (see Fig. 5), which demonstrates the\n",
      "generalizability of our model across datasets. Results of Outdoor Pose Estimation. As shown in Tab.\n",
      "3, LoFTR outperforms the detector-free method DRC-Net by 61% at AUC@10°, demonstrating the\n",
      "effectiveness of the Transformer. For SuperGlue, we use the setup from the open-sourced localization\n",
      "toolbox HLoc [36]. LoFTR out- performs SuperGlue by a large margin (13% at AUC@10°), which\n",
      "demonstrates the effectiveness of the detector-free design. Different from indoor scenes, LoFTR-DS\n",
      "performs better than LoFTR-OT on MegaDepth. More qualitative re- sults can be found in Fig. 5. 4.3.\n",
      "Visual Localization Visual Localization. Besides achieving competitive per- formance for relative\n",
      "pose estimation, LoFTR can also ben- eﬁt visual localization, which is the task to estimate the 6-\n",
      "DoF poses of given images with respect to the correspond- ing 3D scene model. We evaluate LoFTR on\n",
      "the Long-Term Visual Localization Benchmark [43] (referred to as Vis- Loc benchmark in the\n",
      "following). It focuses on benchmark- ing visual localization methods under varying conditions, e.g.,\n",
      "day-night changes, scene geometry changes, and in- door scenes with plenty of texture-less areas.\n",
      "Thus, the vi-\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 6}\n",
      "eﬁt visual localization, which is the task to estimate the 6- DoF poses of given images with respect\n",
      "to the correspond- ing 3D scene model. We evaluate LoFTR on the Long-Term Visual Localization\n",
      "Benchmark [43] (referred to as Vis- Loc benchmark in the following). It focuses on benchmark- ing\n",
      "visual localization methods under varying conditions, e.g., day-night changes, scene geometry\n",
      "changes, and in- door scenes with plenty of texture-less areas. Thus, the vi- sual localization task\n",
      "relies on highly robust image match- ing methods.MethodDUC1 DUC2 (0.25m,10°) / (0.5m,10°) /\n",
      "(1.0m,10°) ISRF [29] 39.4 / 58.1 / 70.2 41.2 / 61.1 / 69.5 KAPTURE [14]+R2D2 [32] 41.4 / 60.1 / 73.7\n",
      "47.3 / 67.2 / 73.3 HLoc [36]+SP [9]+SuperGlue [37] 49.0 / 68.7 / 80.8 53.4 / 77.1 / 82.4 HLoc [36]+\n",
      "LoFTR-OT 47.5 / 72.2 /84.8 54.2 / 74.8 / 85.5 Table 5: Visual localization evaluation on the InLoc\n",
      "[41] benchmark . MethodPose estimation AUC @5° @10° @20° 1) replace LoFTR with convolution 14.98\n",
      "32.04 49.92 2)1/16coarse-resolution + 1/4ﬁne-resolution 16.75 34.82 54.0 3) positional encoding per\n",
      "layer 18.02 35.64 52.77 4) larger model with Nc= 8;Nf= 2 20.87 40.23 57.56 Full (Nc= 4;Nf= 1) 20.06\n",
      "40.8 57.62 Table 6: Ablation study. Five variants of LoFTR are trained and evaluated both on the\n",
      "ScanNet dataset. Evaluation. We evaluate LoFTR on two tracks of VisLoc that consist of several\n",
      "challenges. First, the “visual local- ization for handheld devices” track requires a full localiza-\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 6}\n",
      "2)1/16coarse-resolution + 1/4ﬁne-resolution 16.75 34.82 54.0 3) positional encoding per layer 18.02\n",
      "35.64 52.77 4) larger model with Nc= 8;Nf= 2 20.87 40.23 57.56 Full (Nc= 4;Nf= 1) 20.06 40.8 57.62\n",
      "Table 6: Ablation study. Five variants of LoFTR are trained and evaluated both on the ScanNet\n",
      "dataset. Evaluation. We evaluate LoFTR on two tracks of VisLoc that consist of several challenges.\n",
      "First, the “visual local- ization for handheld devices” track requires a full localiza- tion\n",
      "pipeline. It benchmarks on two datasets, the Aachen- Day-Night dataset [38, 54] concerning outdoor\n",
      "scenes and the InLoc [41] dataset concerning indoor scenes. We use open-sourced localization\n",
      "pipeline HLoc [36] with the matches extracted by LoFTR. Second, the “local features for long-term\n",
      "localization” track provides a ﬁxed localiza- tion pipeline to evaluate the local feature extractors\n",
      "them- selves and optionally the matchers. This track uses the Aachen v1.1 dataset [54]. We provide\n",
      "the implementation details of testing LoFTR on VisLoc in the supplementary material. Results. We\n",
      "provide evaluation results of LoFTR in Tab. 4 and Tab. 5. We have evaluated LoFTR pairing with\n",
      "either the optimal transport layer or the dual-softmax operator and report the one with better\n",
      "results. LoFTR-DS outperforms all baselines in the local feature challenge track, showing its\n",
      "robustness under day-night changes. Then, for the vi- sual localization for handheld devices track,\n",
      "LoFTR-OT\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 6}\n",
      "details of testing LoFTR on VisLoc in the supplementary material. Results. We provide evaluation\n",
      "results of LoFTR in Tab. 4 and Tab. 5. We have evaluated LoFTR pairing with either the optimal\n",
      "transport layer or the dual-softmax operator and report the one with better results. LoFTR-DS\n",
      "outperforms all baselines in the local feature challenge track, showing its robustness under day-\n",
      "night changes. Then, for the vi- sual localization for handheld devices track, LoFTR-OT outperforms\n",
      "all published methods on the challenging In- Loc dataset, which contains extensive appearance\n",
      "changes, more texture-less areas, symmetric and repetitive elements. We attribute the prominence to\n",
      "the use of the Transformer and the optimal transport layer, taking advantage of global information\n",
      "and jointly bringing global consensus into the ﬁnal matches. The detector-free design also plays a\n",
      "criti- cal role, preventing the repeatability problem of detector- based methods in low-texture\n",
      "regions. LoFTR-OT performs on par with the state-of-the-art method SuperPoint + Su- perGlue on night\n",
      "queries of the Aachen v1.1 dataset and slightly worse on the day queries. 7\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 7}\n",
      "IndoorOutdoorSuperPoint + SuperGlueDRC-NetLoFTR Figure 5: Qualitative results. LoFTR is compared to\n",
      "SuperGlue [37] and DRC-Net [19] in indoor and outdoor environ- ments. LoFTR obtains more correct\n",
      "matches and fewer mismatches, successfully coping with low-texture regions and large viewpoint and\n",
      "illumination changes. The red color indicates epipolar error beyond 5\u000210\u00004for indoor scenes and\n",
      "1\u000210\u00004 for outdoor scenes (in the normalized image coordinates). More qualitative results can be\n",
      "found on the project webpage. SelfCross Feature PCA Figure 6: Visualization of self and cross\n",
      "attention weights and the transformed features. In the ﬁrst two examples, the query point from the\n",
      "low-texture region is able to aggregate the surrounding global information ﬂexibly. For instance,\n",
      "the point on the chair is looking at the edge of the chair. In the last two examples, the query\n",
      "point from the distinctive region can also utilize the richer information from other regions. The\n",
      "feature visualization with PCA further shows that LoFTR learns a position-dependent feature\n",
      "representation. 4.4. Understanding LoFTR Ablation Study. To fully understand the different modules\n",
      "in LoFTR, we evaluate ﬁve different variants with results shown in Tab. 6: 1) Replacing the LoFTR\n",
      "module by con- volution with a comparable number of parameters results in a signiﬁcant drop in AUC\n",
      "as expected. 2) Using a smaller version of LoFTR with 1/16and 1/4resolution feature maps at\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 7}\n",
      "position-dependent feature representation. 4.4. Understanding LoFTR Ablation Study. To fully\n",
      "understand the different modules in LoFTR, we evaluate ﬁve different variants with results shown in\n",
      "Tab. 6: 1) Replacing the LoFTR module by con- volution with a comparable number of parameters\n",
      "results in a signiﬁcant drop in AUC as expected. 2) Using a smaller version of LoFTR with 1/16and\n",
      "1/4resolution feature maps at the coarse and ﬁne level, respectively, results in a running time of\n",
      "104 ms and a degraded pose estimation accuracy. 3) Using DETR-style [3] Transformer architecture\n",
      "which has positional encoding at each layer, leads to a noticeably de- clined result. 4) Increasing\n",
      "the model capacity by doubling the number of LoFTR layers to Nc= 8andNf= 2barely changes the\n",
      "results. We conduct these experiments using the same training and evaluation protocol as indoor pose\n",
      "estimation on ScanNet with an optimal transport layer for matching. Visualizing Attention. We\n",
      "visualize the attention weights in Fig. 6.5. Conclusion This paper presents a novel detector-free\n",
      "matching ap- proach, named LoFTR, that can establish accurate semi- dense matches with Transformers\n",
      "in a coarse-to-ﬁne man- ner. The proposed LoFTR module uses the self and cross attention layers in\n",
      "Transformers to transform the local fea- tures to be context- and position-dependent, which is\n",
      "crucial for LoFTR to obtain high-quality matches on indistinctive regions with low-texture or\n",
      "repetitive patterns. Our exper-\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 7}\n",
      "in Fig. 6.5. Conclusion This paper presents a novel detector-free matching ap- proach, named LoFTR,\n",
      "that can establish accurate semi- dense matches with Transformers in a coarse-to-ﬁne man- ner. The\n",
      "proposed LoFTR module uses the self and cross attention layers in Transformers to transform the\n",
      "local fea- tures to be context- and position-dependent, which is crucial for LoFTR to obtain high-\n",
      "quality matches on indistinctive regions with low-texture or repetitive patterns. Our exper- iments\n",
      "show that LoFTR achieves state-of-the-art perfor- mances on relative pose estimation and visual\n",
      "localization on multiple datasets. We believe that LoFTR provides a new direction for detector-free\n",
      "methods in local image feature matching and can be extended to more challenging scenar- ios, e.g.,\n",
      "matching images with severe seasonal changes. Acknowledgement. The authors would like to acknowl-\n",
      "edge the support from the National Key Research and Development Program of China (No.\n",
      "2020AAA0108901), NSFC (No. 61806176), and ZJU-SenseTime Joint Lab of 3D Vision. 8\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 8}\n",
      "References [1] Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krys- tian Mikolajczyk. HPatches:\n",
      "A benchmark and evaluation of handcrafted and learned local descriptors. In CVPR , 2017. 5 [2]\n",
      "JiaWang Bian, Wen-Yan Lin, Yasuyuki Matsushita, Sai-Kit Yeung, Tan-Dat Nguyen, and Ming-Ming Cheng.\n",
      "GMS: Grid-based motion statistics for fast, ultra-robust feature cor- respondence. In CVPR , 2017. 6\n",
      "[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\n",
      "Sergey Zagoruyko. End-to- end object detection with transformers. In ECCV , 2020. 3, 4, 8 [4] Luca\n",
      "Cavalli, Viktor Larsson, Martin Ralf Oswald, Torsten Sattler, and Marc Pollefeys. Handcrafted\n",
      "Outlier Detection Revisited. In ECCV , 2020. 7 [5] Krzysztof Choromanski, Valerii Likhosherstov,\n",
      "David Do- han, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz\n",
      "Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. ICLR , 2021. 3 [6]\n",
      "Christopher B Choy, JunYoung Gwak, Silvio Savarese, and Manmohan Chandraker. Universal\n",
      "correspondence network. NeurIPS , 2016. 2 [7] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\n",
      "ber, Thomas Funkhouser, and Matthias Nießner. ScanNet: Richly-annotated 3d reconstructions of indoor\n",
      "scenes. In CVPR , 2017. 5, 6 [8] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi- novich. Toward\n",
      "geometric deep slam. arXiv:1707.07410 . 2 [9] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 8}\n",
      "3 [6] Christopher B Choy, JunYoung Gwak, Silvio Savarese, and Manmohan Chandraker. Universal\n",
      "correspondence network. NeurIPS , 2016. 2 [7] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\n",
      "ber, Thomas Funkhouser, and Matthias Nießner. ScanNet: Richly-annotated 3d reconstructions of indoor\n",
      "scenes. In CVPR , 2017. 5, 6 [8] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi- novich. Toward\n",
      "geometric deep slam. arXiv:1707.07410 . 2 [9] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-\n",
      "novich. SuperPoint: Self-supervised interest point detection and description. In CVPRW , 2018. 2, 6,\n",
      "7 [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\n",
      "Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is\n",
      "worth 16x16 words: Trans- formers for image recognition at scale. ICLR , 2021. 3 [11] Mihai Dusmanu,\n",
      "Ignacio Rocco, Tomas Pajdla, Marc Polle- feys, Josef Sivic, Akihiko Torii, and Torsten Sattler.\n",
      "D2-Net: A trainable cnn for joint detection and description of local features. CVPR , 2019. 2, 6\n",
      "[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\n",
      "recognition. In CVPR , 2016. 5 [13] Jared Heinly, Enrique Dunn, and Jan-Michael Frahm. Com- parative\n",
      "evaluation of binary features. In ECCV , 2012. 7 [14] Martin Humenberger, Yohann Cabon, Nicolas\n",
      "Guerin, Julien Morat, J ´erˆome Revaud, Philippe Rerole, No ´e Pion, Cesar\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 8}\n",
      "A trainable cnn for joint detection and description of local features. CVPR , 2019. 2, 6 [12]\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\n",
      "In CVPR , 2016. 5 [13] Jared Heinly, Enrique Dunn, and Jan-Michael Frahm. Com- parative evaluation\n",
      "of binary features. In ECCV , 2012. 7 [14] Martin Humenberger, Yohann Cabon, Nicolas Guerin, Julien\n",
      "Morat, J ´erˆome Revaud, Philippe Rerole, No ´e Pion, Cesar de Souza, Vincent Leroy, and Gabriela\n",
      "Csurka. Robust Image Retrieval-based Visual Localization using Kapture. arXiv:2007.13867 . 7 [15]\n",
      "Wei Jiang, Eduard Trulls, Jan Hosang, Andrea Tagliasacchi, and Kwang Moo Yi. COTR: Correspondence\n",
      "Transformer for Matching Across Images, 2021. 2[16] Chaitanya Joshi. Transformers are Graph Neural\n",
      "Networks. https://thegradient.pub/transformers-are-graph- neural-networks/ , 2020. 2 [17] Angelos\n",
      "Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc ¸ois Fleuret. Transformers are RNNs: Fast\n",
      "autoregres- sive transformers with linear attention. In ICML , 2020. 3, 4 [18] Nikita Kitaev, Łukasz\n",
      "Kaiser, and Anselm Levskaya. Re- former: The efﬁcient transformer. ICLR , 2020. 3 [19] Xinghui Li,\n",
      "Kai Han, Shuda Li, and Victor Prisacariu. Dual- resolution correspondence networks. NeurIPS , 2020.\n",
      "1, 2, 6, 7, 8 [20] Zhaoshuo Li, Xingtong Liu, Francis X Creighton, Russell H Taylor, and Mathias\n",
      "Unberath. Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 8}\n",
      "sive transformers with linear attention. In ICML , 2020. 3, 4 [18] Nikita Kitaev, Łukasz Kaiser, and\n",
      "Anselm Levskaya. Re- former: The efﬁcient transformer. ICLR , 2020. 3 [19] Xinghui Li, Kai Han,\n",
      "Shuda Li, and Victor Prisacariu. Dual- resolution correspondence networks. NeurIPS , 2020. 1, 2, 6,\n",
      "7, 8 [20] Zhaoshuo Li, Xingtong Liu, Francis X Creighton, Russell H Taylor, and Mathias Unberath.\n",
      "Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers.\n",
      "arXiv:2011.02910 . 3 [21] Zhengqi Li and Noah Snavely. Megadepth: Learning single- view depth\n",
      "prediction from internet photos. In CVPR , 2018. 5, 6 [22] Tsung-Yi Lin, Piotr Doll ´ar, Ross B.\n",
      "Girshick, Kaiming He, Bharath Hariharan, and Serge J. Belongie. Feature Pyramid Networks for Object\n",
      "Detection. CVPR , 2017. 3 [23] Ce Liu, Jenny Yuen, and Antonio Torralba. SIFT Flow: Dense\n",
      "correspondence across scenes and its applications. T- PAMI , 2010. 2 [24] X. Liu, Y . Zheng, B.\n",
      "Killeen, M. Ishii, G. D. Hager, R. H. Taylor, and M. Unberath. Extremely Dense Point Correspon-\n",
      "dences Using a Learned Feature Descriptor. In CVPR , 2020. 2 [25] Yuan Liu, Zehong Shen, Zhixuan\n",
      "Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. GIFT: Learning transformation-invariant dense visual\n",
      "descriptors via group cnns. NeurIPS , 2019. 2 [26] David G Lowe. Distinctive image features from\n",
      "scale- invariant keypoints. IJCV , 2004. 2, 6 [27] Zixin Luo, Tianwei Shen, Lei Zhou, Jiahui Zhang,\n",
      "Yao Yao,\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 8}\n",
      "Taylor, and M. Unberath. Extremely Dense Point Correspon- dences Using a Learned Feature Descriptor.\n",
      "In CVPR , 2020. 2 [25] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou.\n",
      "GIFT: Learning transformation-invariant dense visual descriptors via group cnns. NeurIPS , 2019. 2\n",
      "[26] David G Lowe. Distinctive image features from scale- invariant keypoints. IJCV , 2004. 2, 6\n",
      "[27] Zixin Luo, Tianwei Shen, Lei Zhou, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, and Long Quan.\n",
      "ContextDesc: Lo- cal Descriptor Augmentation with Cross-Modality Context. CVPR , 2019. 6 [28] Zixin\n",
      "Luo, Lei Zhou, Xuyang Bai, Hongkai Chen, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, and Long Quan.\n",
      "ASLFeat: Learning local features of accurate shape and lo- calization. In CVPR , 2020. 2 [29]\n",
      "Iaroslav Melekhov, Gabriel J Brostow, Juho Kannala, and Daniyar Turmukhambetov. Image Stylization\n",
      "for Robust Features. arXiv:2008.06959 . 7 [30] Krystian Mikolajczyk and Cordelia Schmid. A\n",
      "performance evaluation of local descriptors. T-PAMI , 2005. 7 [31] R ´emi Pautrat, Viktor Larsson,\n",
      "Martin R Oswald, and Marc Pollefeys. Online Invariance Selection for Local Feature De- scriptors. In\n",
      "ECCV , 2020. 7 [32] Jerome Revaud, Philippe Weinzaepfel, C ´esar De Souza, Noe Pion, Gabriela\n",
      "Csurka, Yohann Cabon, and Martin Humen- berger. R2D2: repeatable and reliable detector and descrip-\n",
      "tor.NeurIPS , 2019. 2, 6, 7 [33] Ignacio Rocco, Relja Arandjelovi ´c, and Josef Sivic. Efﬁcient\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 8}\n",
      "evaluation of local descriptors. T-PAMI , 2005. 7 [31] R ´emi Pautrat, Viktor Larsson, Martin R\n",
      "Oswald, and Marc Pollefeys. Online Invariance Selection for Local Feature De- scriptors. In ECCV ,\n",
      "2020. 7 [32] Jerome Revaud, Philippe Weinzaepfel, C ´esar De Souza, Noe Pion, Gabriela Csurka,\n",
      "Yohann Cabon, and Martin Humen- berger. R2D2: repeatable and reliable detector and descrip-\n",
      "tor.NeurIPS , 2019. 2, 6, 7 [33] Ignacio Rocco, Relja Arandjelovi ´c, and Josef Sivic. Efﬁcient\n",
      "neighbourhood consensus networks via submanifold sparse convolutions. In ECCV , 2020. 1, 2, 6, 7 9\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 9}\n",
      "[34] Ignacio Rocco, Mircea Cimpoi, Relja Arandjelovi ´c, Akihiko Torii, Tomas Pajdla, and Josef\n",
      "Sivic. Neighbourhood con- sensus networks. NeurIPS , 2018. 1, 2, 5, 7 [35] Ethan Rublee, Vincent\n",
      "Rabaud, Kurt Konolige, and Gary Bradski. ORB: An efﬁcient alternative to SIFT or SURF. InICCV ,\n",
      "2011. 2, 6 [36] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse\n",
      "to ﬁne: Robust hierarchical localization at large scale. In CVPR , 2019. 7 [37] Paul-Edouard Sarlin,\n",
      "Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. SuperGlue: Learning feature matching with\n",
      "graph neural networks. In CVPR , 2020. 1, 2, 4, 5, 6, 7, 8 [38] Torsten Sattler, Tobias Weyand,\n",
      "Bastian Leibe, and Leif Kobbelt. Image Retrieval for Image-Based Localization Re- visited. In BMVC ,\n",
      "2012. 7 [39] Tanner Schmidt, Richard Newcombe, and Dieter Fox. Self- supervised visual descriptor\n",
      "learning for dense correspon- dence. RAL, 2016. 2 [40] Johannes L Schonberger and Jan-Michael Frahm.\n",
      "Structure- from-Motion revisited. In CVPR , 2016. 6 [41] Hajime Taira, Masatoshi Okutomi, Torsten\n",
      "Sattler, Mircea Cimpoi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, and Ak- ihiko Torii. InLoc:\n",
      "Indoor visual localization with dense matching and view synthesis. In CVPR , 2018. 7 [42] Yi Tay,\n",
      "Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey. arXiv:2009.06732\n",
      ". 3 [43] Carl Toft, Will Maddern, Akihiko Torii, Lars Hammarstrand, Erik Stenborg, Daniel Safari,\n",
      "Masatoshi Okutomi, Marc\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 9}\n",
      "[41] Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea Cimpoi, Marc Pollefeys, Josef Sivic,\n",
      "Tomas Pajdla, and Ak- ihiko Torii. InLoc: Indoor visual localization with dense matching and view\n",
      "synthesis. In CVPR , 2018. 7 [42] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient\n",
      "transformers: A survey. arXiv:2009.06732 . 3 [43] Carl Toft, Will Maddern, Akihiko Torii, Lars\n",
      "Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi Okutomi, Marc Pollefeys, Josef Sivic, Tomas\n",
      "Pajdla, et al. Long-Term Vi- sual Localization Revisited. T-PAMI , 2020. 7 [44] Prune Truong, Martin\n",
      "Danelljan, L. Gool, and R. Timo- fte. Learning Accurate Dense Correspondences and When to Trust\n",
      "Them. ArXiv , abs/2101.01710, 2021. 2 [45] Prune Truong, Martin Danelljan, Luc Van Gool, and Radu\n",
      "Timofte. GOCor: Bringing Globally Optimized Correspon- dence V olumes into Your Neural Network. In\n",
      "NeurIPS , 2020. 2 [46] Prune Truong, Martin Danelljan, and Radu Timofte. GLU- Net: Global-Local\n",
      "Universal Network for dense ﬂow and correspondences. In CVPR , 2020. 2 [47] Michał Tyszkiewicz,\n",
      "Pascal Fua, and Eduard Trulls. DISK: Learning local features with policy gradient. NeurIPS , 2020.\n",
      "2, 5, 6 [48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N\n",
      "Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS , 2017. 2, 3 [49]\n",
      "Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-\n",
      "Deeplab: Stand-\n",
      "==================\n",
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 9}\n",
      "correspondences. In CVPR , 2020. 2 [47] Michał Tyszkiewicz, Pascal Fua, and Eduard Trulls. DISK:\n",
      "Learning local features with policy gradient. NeurIPS , 2020. 2, 5, 6 [48] Ashish Vaswani, Noam\n",
      "Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\n",
      "Polosukhin. Attention is all you need. NeurIPS , 2017. 2, 3 [49] Huiyu Wang, Yukun Zhu, Bradley\n",
      "Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-Deeplab: Stand- alone axial-attention\n",
      "for panoptic segmentation. In ECCV , 2020. 3 [50] Qianqian Wang, Xiaowei Zhou, Bharath Hariharan,\n",
      "and Noah Snavely. Learning feature descriptors using camera pose supervision. In ECCV , 2020. 5 [51]\n",
      "Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal Fua. LIFT: Learned invariant feature\n",
      "transform. In ECCV , 2016. 2[52] Kwang Moo Yi, Eduard Trulls, Yuki Ono, Vincent Lepetit, Mathieu\n",
      "Salzmann, and Pascal Fua. Learning to ﬁnd good correspondences. In CVPR , 2018. 6 [53] Jiahui Zhang,\n",
      "Dawei Sun, Zixin Luo, Anbang Yao, Lei Zhou, Tianwei Shen, Yurong Chen, Long Quan, and Hongen Liao.\n",
      "Learning Two-View Correspondences and Geometry Using Order-Aware Network. ICCV , 2019. 6 [54] Zichao\n",
      "Zhang, Torsten Sattler, and Davide Scaramuzza. Ref- erence Pose Generation for Long-term Visual\n",
      "Localization via Learned Features and View Synthesis. IJCV , 2020. 7 10\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "# Show the chunks\n",
    "for d in docs:\n",
    "    print(\"==================\")\n",
    "    print(d.metadata)\n",
    "    printw(d.page_content)\n",
    "    print(\"==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "{'source': 'data/LoFTR_paper.pdf', 'page': 4}\n",
      "c: Lc=\u00001 jMgt cjX (~i;~j)2Mgt clogPc\u0000~i;~j\u0001 : Fine-level Supervision. We use the`2loss for ﬁne-level\n",
      "reﬁnement. Following [50], for each query point ^i, we also measure its uncertainty by calculating\n",
      "the total variance \u001b2(^i)of the corresponding heatmap. The target is to opti- mize the reﬁned\n",
      "position that has low uncertainty, resulting in the ﬁnal weighted loss function: Lf=1 jMfjX\n",
      "(^i;^j0)2M f1 \u001b2(^i)   ^j0\u0000^j0 gt    2; in which ^j0 gtis calculated by warping each ^ifrom ^FA\n",
      "tr(^i)to ^FB tr(^j)with the ground-truth camera pose and depth. We ignore ( ^i,^j0) if the warped\n",
      "location of ^ifalls out of the local window of ^FB tr(^j)when calculatingLf. The gradient is not\n",
      "backpropagated through \u001b2(^i)during training. 3.6. Implementation Details We train the indoor model\n",
      "of LoFTR on the ScanNet [7] dataset and the outdoor model on the MegaDepth [21] fol- lowing [37]. On\n",
      "ScanNet, the model is trained using Adam with an initial learning rate of 1\u000210\u00003and a batch size of\n",
      "64. It converges after 24 hours of training on 64 GTX 1080Ti GPUs. The local feature CNN uses a\n",
      "modiﬁed ver- sion of ResNet-18 [12] as the backbone. The entire model is trained end-to-end with\n",
      "randomly initialized weights. Nc is set to 4 and Nfis 1.\u0012cis chosen to 0.2. Window size wis equal to\n",
      "5. ~FA trand~FB trare upsampled and concate- nated with ^FAand^FBbefore passing through the ﬁne-\n",
      "level LoFTR in the implementation. The full model with dual- softmax matching runs at 116 ms for a\n",
      "640 \u0002480 image pair\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "# Show the chunks\n",
    "for d in docs:\n",
    "    if \"batch size\" in d.page_content:\n",
    "        print(\"==================\")\n",
    "        print(d.metadata)\n",
    "        printw(d.page_content)\n",
    "        print(\"==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=None, openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6, request_timeout=None, headers=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the embeddings class\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Chroma vectorstore\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "docsearch = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever()\n",
    "# Settings from https://python.langchain.com/en/latest/use_cases/code/code-analysis-deeplake.html\n",
    "# Explanation of MMR: https://python.langchain.com/en/latest/modules/prompts/example_selectors/examples/mmr.html\n",
    "# retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "# retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "# retriever.search_kwargs['fetch_k'] = 20  # Number of Documents to fetch to pass to MMR algorithm\n",
    "# retriever.search_kwargs['k'] = 5  # Number of Documents to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT3.5-turbo model.\n"
     ]
    }
   ],
   "source": [
    "use_chat_model = True\n",
    "\n",
    "\n",
    "if use_chat_model:\n",
    "    # GPT3.5-turbo\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "    llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0) # 'ada' 'gpt-3.5-turbo' 'gpt-4',\n",
    "    qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, return_source_documents=True)\n",
    "\n",
    "    print(\"Loaded GPT3.5-turbo model.\")\n",
    "else:\n",
    "    # GPT3\n",
    "    from langchain.llms import OpenAI\n",
    "    from langchain.chains.question_answering import load_qa_chain\n",
    "    from langchain.chains import RetrievalQA\n",
    "\n",
    "    llm = OpenAI(temperature=0)\n",
    "    # qa = load_qa_chain(llm)\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "\n",
    "    print(\"Loaded GPT3 model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: What learning rate is used for training? \n",
      "\n",
      "**Answer**:\n",
      "The indoor model of LoFTR is trained using Adam with an initial learning rate of 1e-3 and a batch si\n",
      "ze of 64.\n",
      "\n",
      "**Sources**:\n",
      " 1. Page 5\n",
      "c:\n",
      "Lc=\u00001\n",
      "jMgt\n",
      "cjX\n",
      "(~i;~j)2Mgt\n",
      "clogPc\u0000~i;~j\u0001\n",
      ":\n",
      "Fine-level Supervision. We use the`2loss for ﬁne-level\n",
      "reﬁnement. Following [50], for each query point ^\n",
      "\n",
      " 2. Page 5\n",
      "1080Ti GPUs. The local feature CNN uses a modiﬁed ver-\n",
      "sion of ResNet-18 [12] as the backbone. The entire model\n",
      "is trained end-to-end with randomly in\n",
      "\n",
      " 3. Page 2\n",
      "tures have achieved good performances. SIFT [26] and\n",
      "ORB [35] are arguably the most successful hand-crafted\n",
      "local features and are widely adopted in m\n",
      "\n",
      " 4. Page 7\n",
      "2)1/16coarse-resolution + 1/4ﬁne-resolution 16.75 34.82 54.0\n",
      "3) positional encoding per layer 18.02 35.64 52.77\n",
      "4) larger model with Nc= 8;Nf= 2 20.87\n",
      "\n",
      " 1. Page 5\n",
      "c:\n",
      "Lc=\u00001\n",
      "jMgt\n",
      "cjX\n",
      "(~i;~j)2Mgt\n",
      "clogPc\u0000~i;~j\u0001\n",
      ":\n",
      "Fine-level Supervision. We use the`2loss for ﬁne-level\n",
      "reﬁnement. Following [50], for each query point ^i, we\n",
      "also measure its uncertainty by calculating the total variance\n",
      "\u001b2(^i)of the corresponding heatmap. The target is to opti-\n",
      "mize the reﬁned position that has low uncertainty, resulting\n",
      "in the ﬁnal weighted loss function:\n",
      "Lf=1\n",
      "jMfjX\n",
      "(^i;^j0)2M f1\n",
      "^j0\u0000^j0\n",
      "gt\n",
      "2;\n",
      "in which ^j0\n",
      "gtis calculated by warping each ^ifrom ^FA\n",
      "tr(^i)to\n",
      "^FB\n",
      "tr(^j)with the ground-truth camera pose and depth. We\n",
      "ignore ( ^i,^j0) if the warped location of ^ifalls out of the local\n",
      "window of ^FB\n",
      "tr(^j)when calculatingLf. The gradient is not\n",
      "backpropagated through \u001b2(^i)during training.\n",
      "3.6. Implementation Details\n",
      "We train the indoor model of LoFTR on the ScanNet [7]\n",
      "dataset and the outdoor model on the MegaDepth [21] fol-\n",
      "lowing [37]. On ScanNet, the model is trained using Adam\n",
      "with an initial learning rate of 1\u000210\u00003and a batch size\n",
      "of 64. It converges after 24 hours of training on 64 GTX\n",
      "1080Ti GPUs. The local feature CNN uses a modiﬁed ver-\n",
      "sion of ResNet-18 [12] as the backbone. The entire model\n",
      "is trained end-to-end with randomly initialized weights. Nc\n",
      "is set to 4 and Nfis 1.\u0012cis chosen to 0.2. Window size\n",
      "wis equal to 5. ~FA\n",
      "trand~FB\n",
      "trare upsampled and concate-\n",
      "nated with ^FAand^FBbefore passing through the ﬁne-level\n",
      "LoFTR in the implementation. The full model with dual-\n",
      "softmax matching runs at 116 ms for a 640 \u0002480 image pair\n",
      "\n",
      " 2. Page 5\n",
      "1080Ti GPUs. The local feature CNN uses a modiﬁed ver-\n",
      "sion of ResNet-18 [12] as the backbone. The entire model\n",
      "is trained end-to-end with randomly initialized weights. Nc\n",
      "is set to 4 and Nfis 1.\u0012cis chosen to 0.2. Window size\n",
      "wis equal to 5. ~FA\n",
      "trand~FB\n",
      "trare upsampled and concate-\n",
      "nated with ^FAand^FBbefore passing through the ﬁne-level\n",
      "LoFTR in the implementation. The full model with dual-\n",
      "softmax matching runs at 116 ms for a 640 \u0002480 image pair\n",
      "on an RTX 2080Ti. Under the optimal transport setup, we\n",
      "use three sinkhorn iterations, and the model runs at 130 ms.\n",
      "We refer readers to the supplementary material for more de-\n",
      "tails of training and timing analyses.\n",
      "4. Experiments\n",
      "4.1. Homography Estimation\n",
      "In the ﬁrst experiment, we evaluate LoFTR on the widely\n",
      "adopted HPatches dataset [1] for homography estimation.\n",
      "HPatches contains 52 sequences under signiﬁcant illumina-\n",
      "tion changes and 56 sequences that exhibit large variation\n",
      "in viewpoints.\n",
      "5\n",
      "\n",
      " 3. Page 2\n",
      "tures have achieved good performances. SIFT [26] and\n",
      "ORB [35] are arguably the most successful hand-crafted\n",
      "local features and are widely adopted in many 3D com-\n",
      "puter vision tasks. The performance on large viewpoint\n",
      "and illumination changes of local features can be signif-\n",
      "icantly improved with learning-based methods. Notably,\n",
      "LIFT [51] and MagicPoint [8] are among the ﬁrst success-\n",
      "ful learning-based local features. They adopt the detector-\n",
      "based design in hand-crafted methods and achieve good\n",
      "performance. SuperPoint [9] builds upon MagicPoint and\n",
      "proposes a self-supervised training method through homo-\n",
      "graphic adaptation. Many learning-based local features\n",
      "along this line [32, 11, 25, 28, 47] also adopt the detector-\n",
      "based design.The above-mentioned local features use the nearest\n",
      "neighbor search to ﬁnd matches between the extracted in-\n",
      "terest points. Recently, SuperGlue [37] proposes a learning-\n",
      "based approach for local feature matching. SuperGlue ac-\n",
      "cepts two sets of interest points with their descriptors as\n",
      "input and learns their matches with a graph neural net-\n",
      "work (GNN), which is a general form of Transformers [16].\n",
      "Since the priors in feature matching can be learned with a\n",
      "data-driven approach, SuperGlue achieves impressive per-\n",
      "formance and sets the new state of the art in local feature\n",
      "matching. However, being a detector-dependent method,\n",
      "it has the fundamental drawback of being unable to detect\n",
      "repeatable interest points in indistinctive regions. The at-\n",
      "\n",
      " 4. Page 7\n",
      "2)1/16coarse-resolution + 1/4ﬁne-resolution 16.75 34.82 54.0\n",
      "3) positional encoding per layer 18.02 35.64 52.77\n",
      "4) larger model with Nc= 8;Nf= 2 20.87 40.23 57.56\n",
      "Full (Nc= 4;Nf= 1) 20.06 40.8 57.62\n",
      "Table 6: Ablation study. Five variants of LoFTR are\n",
      "trained and evaluated both on the ScanNet dataset.\n",
      "Evaluation. We evaluate LoFTR on two tracks of VisLoc\n",
      "that consist of several challenges. First, the “visual local-\n",
      "ization for handheld devices” track requires a full localiza-\n",
      "tion pipeline. It benchmarks on two datasets, the Aachen-\n",
      "Day-Night dataset [38, 54] concerning outdoor scenes and\n",
      "the InLoc [41] dataset concerning indoor scenes. We\n",
      "use open-sourced localization pipeline HLoc [36] with the\n",
      "matches extracted by LoFTR. Second, the “local features\n",
      "for long-term localization” track provides a ﬁxed localiza-\n",
      "tion pipeline to evaluate the local feature extractors them-\n",
      "selves and optionally the matchers. This track uses the\n",
      "Aachen v1.1 dataset [54]. We provide the implementation\n",
      "details of testing LoFTR on VisLoc in the supplementary\n",
      "material.\n",
      "Results. We provide evaluation results of LoFTR in Tab. 4\n",
      "and Tab. 5. We have evaluated LoFTR pairing with either\n",
      "the optimal transport layer or the dual-softmax operator and\n",
      "report the one with better results. LoFTR-DS outperforms\n",
      "all baselines in the local feature challenge track, showing\n",
      "its robustness under day-night changes. Then, for the vi-\n",
      "sual localization for handheld devices track, LoFTR-OT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    # Qs w good answers\n",
    "    # \"What is the LoFTR model?\",\n",
    "    # \"Where can I find the Github source code?\",\n",
    "    # \"Where can I find the Github source code for LoFTR?. Quote the part of the paper where they mention it.\",\n",
    "    # \"What learning rate do they use for training LoFTR?\",\n",
    "    # \"Why is the use of Transformer networks important for LoFTR?\",\n",
    "    \"What learning rate is used for training?\",\n",
    "    # \"What batch size is used for training?\",\n",
    "\n",
    "    # Qs w bad answers\n",
    "    # \"Who is the first author of the LoFTR paper? It's next to the paper title.\", \n",
    "    # \"Who are the authors of the paper?\", \n",
    "    # \"On what page is the conclusion?\"  # does the llm get this information?\n",
    "]\n",
    "\n",
    "def ask_question(question, use_chat_model=False, chat_history=[]):\n",
    "    if use_chat_model:\n",
    "        response = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "        answer = response[\"answer\"]\n",
    "        chat_history.append((question, answer))\n",
    "    else:\n",
    "        # response = qa.run(input_documents=matched_docs, question=question)\n",
    "        response = qa({\"query\": question})\n",
    "        answer = response[\"result\"]\n",
    "    return answer, response[\"source_documents\"]\n",
    "\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**:\")\n",
    "\n",
    "    answer, matched_docs = ask_question(question, use_chat_model=use_chat_model)\n",
    "\n",
    "    print(\"\\n\".join(answer[i:i+100] for i in range(0, len(answer), 100)))\n",
    "    print(\"\\n**Sources**:\")\n",
    "    # Print short sources\n",
    "    for i, doc in enumerate(matched_docs):\n",
    "        print(f\" {i+1}. Page {doc.metadata['page']+1}\\n{doc.page_content[:150]}\\n\")\n",
    "    # Print full sources\n",
    "    for i, doc in enumerate(matched_docs):\n",
    "        print(f\" {i+1}. Page {doc.metadata['page']+1}\\n{doc.page_content}\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
